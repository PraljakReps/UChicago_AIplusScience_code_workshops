{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraljakReps/UChicago_AIplusScience_code_workshops/blob/main/2023_AI%2BScienceSummerSchool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SEqbR8r7r14"
      },
      "source": [
        "# Protein-ligand binding affinity prediction using deep learning üß™ü•ºüíª\n",
        "\n",
        "### Develop and train deep learning architectures to process protein and ligand data for predicting binding affinity\n",
        "\n",
        "Things that we will learn from this notebook:\n",
        "\n",
        "  * **SCIENCE**: understand protein and ligands (e.g. protein primary sequecne, tertiary structure, molecular ligand graph, SMILES strings for molecules).\n",
        "  * **SCIENCE**: understand the importance of binding affinity for protein-ligand complexes\n",
        "  * **AI**: preprocess protein sequence, structure data and molecular graph and ligand SMILES string data.\n",
        "  * **AI**: develop dataloaders in PyTorch to process input data for training various tasks (e.g. protein sequences, SMILES strings, structure).\n",
        "  * **AI**: develop, train, and implement feedforward neural networks for binding affinity prediction for single- and multi- modality tasks (i.e. handle only protein sequences or ligand SMILES strings, the combination of the two, and handle sequence and structure information).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brIbeN5OLV1T"
      },
      "source": [
        "## Illustrations of protein-ligand binding.\n",
        "\n",
        "Here, we show a protein (blue) and ligand (orange) binding into a large complex referred to as protein with binding ligand.\n",
        "![Cartoon illustration of protein-ligand binding](https://upload.wikimedia.org/wikipedia/commons/e/ee/Proten_ligand_binding.PNG?20170819201138)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Aj_-_OK_Ud"
      },
      "source": [
        "Here, we show the tertiary structure of the protein (blue) and ligand (red), where the ligand is binding in the pocket of the protein.\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/Myoglobin_and_heme.png/300px-Myoglobin_and_heme.png)\n",
        "\n",
        "\n",
        "One way to understand the binding between the protein and ligand is to measure the dissociation constant $K_d$. The $K_d$ in the context of a protein-ligand complex, quantifies the affinity between the protein and the ligand. In other words, it represents the concentration of ligand at which half of the protein is in the ligand-bound state at equilibrium.\n",
        "\n",
        "If a protein and ligand interact to form a complex, this can be represented as:\n",
        "\n",
        "P + L ‚áå PL\n",
        "\n",
        "where P is the protein, L is the ligand, and PL is the protein-ligand complex.\n",
        "\n",
        "The Kd is calculated as follows:\n",
        "\n",
        "Kd = [P][L]/[PL]\n",
        "\n",
        "Here, [P], [L], and [PL] are the concentrations of the protein, ligand, and complex, respectively, at equilibrium.\n",
        "\n",
        "**A lower Kd value indicates a higher affinity of the protein for the ligand because less ligand is needed to saturate the protein. Conversely, a higher Kd value suggests a lower affinity, as more ligand is needed to saturate the protein.**\n",
        "\n",
        "It's important to note that the Kd value is a specific measure of the equilibrium between the bound and unbound states of the protein and ligand, and does not provide information about the kinetics of binding or unbinding. It is also independent of the concentration of protein or ligand, assuming these concentrations are in excess over the Kd.\n",
        "## **Goal:**\n",
        " We aim to obtain data about proteins and ligands, either through their sequence details or 3D structural information. We then plan to utilize deep learning techniques to analyze this data and make predictions about their affinities, in other words, their propensity to bind with each other.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G3pxLjwN-RM1"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Check GPU type** üïµÔ∏è\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown V100 = Excellent (*Available only for Colab Pro users*)\n",
        "\n",
        "#@markdown P100 = Very Good\n",
        "\n",
        "#@markdown T4 = Good (*preferred*)\n",
        "\n",
        "#@markdown K80 = Meh\n",
        "\n",
        "#@markdown P4 = (*Not Recommended*)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip show mdtraj"
      ],
      "metadata": {
        "id": "vIO8ibRfd-Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa0_ahZo4afC"
      },
      "outputs": [],
      "source": [
        "#@title Download packages, data, and libraries... üèóÔ∏è\n",
        "# @markdown This cell will take a little while because it has to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "# clone remote repo that is specific to this notebook\n",
        "!git clone https://github.com/PraljakReps/UChicago_AIplusScience_code_workshops.git\n",
        "\n",
        "# install packages\n",
        "!pip install -q mdtraj nglview\n",
        "!pip install rdkit\n",
        "!pip install deepchem\n",
        "!pip install biopython\n",
        "!pip install torch_geometric\n",
        "\n",
        "# download data....\n",
        "!wget https://pdbbind.oss-cn-hangzhou.aliyuncs.com/download/PDBbind_v2020_other_PL.tar.gz\n",
        "# download and prepare datasets...\n",
        "!mkdir /content/v2020\n",
        "\n",
        "!tar -xvzf PDBbind_v2020_other_PL.tar.gz -C /content/v2020\n",
        "\n",
        "# download spreadsheet information\n",
        "!wget https://pdbbind.oss-cn-hangzhou.aliyuncs.com/download/PDBbind_v2020_plain_text_index.tar.gz\n",
        "!tar -xvzf PDBbind_v2020_plain_text_index.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QkwKeOVOmCnU"
      },
      "outputs": [],
      "source": [
        "#@title Import software ‚öôÔ∏è\n",
        "#@markdown Define necessary functions.\n",
        "\n",
        "#@markdown ---\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from rdkit import Chem\n",
        "import mdtraj as md\n",
        "from rdkit import Chem, RDLogger\n",
        "import mdtraj as md\n",
        "import nglview\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "import deepchem as dc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import display, Image\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MFTeUmikm6js"
      },
      "outputs": [],
      "source": [
        "#@title Loading and reading data üì¶\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "with open('./index/INDEX_general_PL_data.2020', 'r') as file:\n",
        "  lines = file.readlines()\n",
        "\n",
        "data = []\n",
        "for line in lines[6:]:\n",
        "    line = line.strip()\n",
        "    fields = line.split()\n",
        "\n",
        "    pdb_code = fields[0]\n",
        "    resolution = fields[1]\n",
        "    release_year = fields[2]\n",
        "    binding_data = fields[3]\n",
        "    affinity_measurement = fields[4]\n",
        "\n",
        "    data.append([pdb_code, resolution, release_year, binding_data, affinity_measurement])\n",
        "\n",
        "columns = [\"PDB code\", \"resolution\", \"release year\", \"binding data\", \"exp_measurement\"]\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "pdb_list = os.listdir('/content/v2020/v2020-other-PL/') # get the pdb complexes that are found in the v2020-other-PL folder...\n",
        "df = df[df['PDB code'].isin(pdb_list)] # only use complexes that are found in the v2020-other-PL folder...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzroh43mBI6O"
      },
      "source": [
        "# Preprocess data into readable format ‚öíÔ∏è\n",
        "\n",
        "The data contains experimental measurements in terms of three variables: dissociation constant $K_d$, inhibition constant $K_i$, and half-maximal inhibitory concentration $IC50$. For our study, we will focus only on protein-ligand complexes with corredponing dissociation constnats $K_d$, however, I will define below each variable to understand the importance of the measurements.\n",
        "\n",
        "- $K_d$ (dissociation constant): This is a measure of the affinity of the protein for its ligand. It is the concentration of ligand at which half of the protein is bound to the ligand at equilibrium. A lower Kd indicates higher affinity (because less ligand is needed to bind half of the protein), while a higher Kd indicates lower affinity.\n",
        "\n",
        "- $K_i$ (inhibition constant): In the context of enzyme kinetics, this is the measure of the potency of an inhibitor, which is a molecule that binds to an enzyme and decreases its activity. The Ki is the concentration of inhibitor that is needed to reduce the binding of a substrate to an enzyme by half in the presence of a given amount of substrate. Just like the Kd, a lower Ki means that the inhibitor has higher affinity for the enzyme, and it takes less inhibitor to achieve half inhibition.\n",
        "\n",
        "- $IC50$ (half-maximal inhibitory concentration): This is a measure of the effectiveness of a substance in inhibiting a specific biological or biochemical function. It is the concentration of an inhibitor where the response (or binding) is reduced by half. IC50 is often used in drug discovery to judge the potency of a substance. It's important to note that IC50 is not a direct measure of affinity or dissociation constant but provides a functional measure of inhibitor potency under specific conditions. It can be related to Ki but the relationship depends on the specific conditions of the assay from which it is derived.\n",
        "\n",
        "If you are interested in further understanding these three variables, feel free to read the following resources:\n",
        "\n",
        " - short blog: https://www.sciencesnail.com/science/the-difference-between-ki-kd-ic50-and-ec50-values\n",
        " - academic paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3000649/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5QdNu0Rovo4"
      },
      "outputs": [],
      "source": [
        "# Kd (dissociation Constant) datasheet\n",
        "Kd_df = df[\n",
        "    df['exp_measurement'].str.contains('Kd')\n",
        "].reset_index(drop=True)\n",
        "\n",
        "# Ki (Inhibition Constant) datasheet\n",
        "Ki_df = df[\n",
        "    df['exp_measurement'].str.contains('Ki')\n",
        "].reset_index(drop=True)\n",
        "\n",
        "# IC50 datasheet\n",
        "IC50_df = df[\n",
        "    df['exp_measurement'].str.contains('IC50')\n",
        "].reset_index(drop=True)\n",
        "\n",
        "print('Datasheet corresponding to protein-ligand complexes with (Kd) dissociation constants.')\n",
        "display(Kd_df.head(3))\n",
        "print('\\n\\nDatasheet corresponding to protein-ligand complexes with (Ki) Inhibition constants.')\n",
        "display(Ki_df.head(3))\n",
        "print('\\n\\nDatasheet corresponding to protein-ligand complexes with (IC50) Half-Maximal Inhibitory Concentration.')\n",
        "display(IC50_df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EY4OykAe-rr0"
      },
      "outputs": [],
      "source": [
        "#@title Function for retrieving ligand and protein structure data paths. üõ†Ô∏è\n",
        "def add_molecule_data_paths(df: pd.Series):\n",
        "\n",
        "  data_path_dict = {\n",
        "      'ligand_mol_paths': [],\n",
        "      'ligand_sdf_paths': [],\n",
        "      'pocket_paths': [],\n",
        "      'protein_pdb_paths': []\n",
        "  }\n",
        "  for pdb_id in tqdm(df['PDB code']):\n",
        "\n",
        "    pdb_path = f'/content/v2020/v2020-other-PL/{pdb_id}/{pdb_id}_'\n",
        "\n",
        "    # track data paths\n",
        "    data_path_dict['ligand_mol_paths'].append( pdb_path + 'ligand.mol2' )\n",
        "    data_path_dict['ligand_sdf_paths'].append( pdb_path + 'ligand.sdf' )\n",
        "    data_path_dict['pocket_paths'].append( pdb_path + 'pocket.pdb' )\n",
        "    data_path_dict['protein_pdb_paths'].append( pdb_path + 'protein.pdb' )\n",
        "\n",
        "  data_path_df = pd.DataFrame(data_path_dict)\n",
        "\n",
        "  # return the final appended dataframe\n",
        "  return pd.concat((df, data_path_df), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MnmXzkGCNIT"
      },
      "outputs": [],
      "source": [
        "' Prepare the dataframe with the path to the data files for the ligand and protein structures.'\n",
        "\n",
        "# attach the protein, ligand data paths\n",
        "Kd_df = add_molecule_data_paths(df=Kd_df)\n",
        "Ki_df = add_molecule_data_paths(df=Ki_df)\n",
        "IC50_df = add_molecule_data_paths(df=IC50_df)\n",
        "\n",
        "# Visualize the added columns that point the protein, ligand sequence/structure data paths...\n",
        "print('Dataframe corresponding to the protein-ligand compex dataframe with (Kd) dissociation constants.')\n",
        "display(Kd_df.head(3))\n",
        "print('\\n\\nDataframe corresponding to the protein-ligand compex dataframe with (Ki) Inhibition constants.')\n",
        "display(Ki_df.head(3))\n",
        "print('\\n\\nDataframe corresponding to the protein-ligand compex dataframe with (IC50) Half-Maximal Inhibitory Concentration.')\n",
        "display(IC50_df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecVYJkTB4ti3"
      },
      "outputs": [],
      "source": [
        "# Load the dataframes with sequence and ligand molecules\n",
        "temp_Kd_df = pd.read_csv('/content/UChicago_AIplusScience_code_workshops/data/final_Kd_dataframe.csv')\n",
        "temp_Ki_df = pd.read_csv('/content/UChicago_AIplusScience_code_workshops/data/final_Ki_dataframe.csv')\n",
        "temp_IC50_df = pd.read_csv('/content/UChicago_AIplusScience_code_workshops/data/final_IC50_dataframe.csv')\n",
        "\n",
        "\n",
        "# match the uploaded dataframe and match the rows with the colab env dataframe\n",
        "# --> processing the final Kd dataframe..\n",
        "final_Kd_df = pd.merge(Kd_df, temp_Kd_df, on='PDB code', suffixes=('', '_x'))\n",
        "final_Kd_df = final_Kd_df[final_Kd_df.columns.drop(list(final_Kd_df.filter(regex='_x')))]\n",
        "final_Kd_df = final_Kd_df.dropna(subset=['ligand_smiles'])\n",
        "\n",
        "# --> processing the final Ki dataframe..\n",
        "final_Ki_df = pd.merge(Ki_df, temp_Ki_df, on='PDB code', suffixes=('', '_x'))\n",
        "final_Ki_df = final_Ki_df[final_Ki_df.columns.drop(list(final_Ki_df.filter(regex='_x')))]\n",
        "final_Ki_df = final_Ki_df.dropna(subset=['ligand_smiles'])\n",
        "\n",
        "# --> processing the final IC50 dataframe..\n",
        "final_IC50_df = pd.merge(IC50_df, temp_IC50_df, on='PDB code', suffixes=('', '_x'))\n",
        "final_IC50_df = final_Ki_df[final_IC50_df.columns.drop(list(final_IC50_df.filter(regex='_x')))]\n",
        "final_IC50_df = final_IC50_df.dropna(subset=['ligand_smiles'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63OUi6_i8Hkl"
      },
      "outputs": [],
      "source": [
        "print('Two new columns, containing a protein sequence and ligand smile sequences.')\n",
        "display(final_Kd_df[['PDB code', 'protein_sequence', 'ligand_smiles']].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU4K2Feqs5Hj"
      },
      "outputs": [],
      "source": [
        "#@title Problem: create a function to convert string to value üö©\n",
        "\n",
        "#@markdown **Goal** of the function `convert_unit_to_value` is to convert experimennt measurement ($K_d$, $K_i$, $IC50$) into a floating point value which can be used for inference and training.\n",
        "\n",
        "#@markdown E.g., convert $str(K_d=250mM)$ into $float(250*1e-3)$\n",
        "\n",
        "#@markdown **Note**: functions from re will be helpful to extracting the numerical value from a given string. We will find this line of code for you as a reference ( re.findall(r'(\\d+(?:\\.\\d+)?)\\s*([a-zA-Z]+)', value)[0]), where value is the $str(K_d=250mM)$. Ref: [re.finalall()](https://www.codecademy.com/resources/docs/python/regex/findall).\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "import re\n",
        "\n",
        "def convert_unit_to_value(value: str) -> float:\n",
        "\n",
        "  # Define a dictionary of conversion factors for different units\n",
        "  conversion_factors = {\n",
        "      'M': 1.0,\n",
        "      'mM':1e-3,\n",
        "      'uM':1e-6,\n",
        "      'nM': 1e-9,\n",
        "      'pM': 1e-12,\n",
        "      'fM': 1e-15\n",
        "  }\n",
        "\n",
        "  # Extract the numerical value and unit\n",
        "  numerical_value, unit = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*([a-zA-Z]+)', value)[0] # e.g. spit kd=255nM into ('255', 'nM').\n",
        "\n",
        "  # apply the conversion factor from the dictionary\n",
        "  converted_value = float(numerical_value) * conversion_factors[unit]\n",
        "  return converted_value / conversion_factors['uM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8w75knouKjm"
      },
      "outputs": [],
      "source": [
        "#@title Convert string-based experimental measurements into floating values.\n",
        "\n",
        "# convert string-based experimental measurements into\n",
        "final_Kd_df['Kd_values[uM]'] = [convert_unit_to_value(value) for value in final_Kd_df['exp_measurement']]\n",
        "final_Ki_df['Ki_values[uM]'] = [convert_unit_to_value(value) for value in final_Ki_df['exp_measurement']]\n",
        "final_IC50_df['IC50_values[uM]'] = [convert_unit_to_value(value) for value in final_IC50_df['exp_measurement']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "stkx_5TVwQku"
      },
      "outputs": [],
      "source": [
        "#@title  Display the dataframe columns that corresponds to the preprocess output measurements\n",
        "\n",
        "print('Kd measurements after preprocessing...')\n",
        "display(final_Kd_df['Kd_values[uM]'].head(3))\n",
        "print('\\n\\nKi measurements after preprocessing...')\n",
        "display(final_Ki_df['Ki_values[uM]'].head(3))\n",
        "print('\\n\\nIC50 measurements after preprocessing...')\n",
        "display(final_IC50_df['IC50_values[uM]'].head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2BBfZgywiln"
      },
      "outputs": [],
      "source": [
        "#@title Split Kd data into training/testing splits:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the Kd dataframe into a train/test split:\n",
        "train_Kd_df, test_Kd_df = train_test_split(final_Kd_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reset index for both dataframes\n",
        "train_Kd_df = train_Kd_df.reset_index(drop=True)\n",
        "test_Kd_df = test_Kd_df.reset_index(drop=True)\n",
        "\n",
        "print('Training and testing dataset sizes:', train_Kd_df.shape[0], test_Kd_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlWqcdfNyu1s"
      },
      "outputs": [],
      "source": [
        "#@title Problem: create a function to normalize experimental data üö©\n",
        "\n",
        "#@markdown Goal of the function `normalize_values` is to normalize $K_d$, $K_i$, and $IC50$. Normalization of input data is a common preprocessing step in machine learning (ML). Here are some reasons why we might normalize affinity measurements in ML:\n",
        "\n",
        "#@markdown - **Easier Optimization**: Some ML algorithms, especially those that use gradient descent for optimization, converge faster when features are on similar scales. For example, in a neural network, having all input features in a similar range allows the network to learn weights more effectively.\n",
        "#@markdown - **Prevent Numerical Instability**: Some ML algorithms can suffer from numerical instability (e.g., underflows and overflows) when working with very large or very small numbers. Normalization can help mitigate these issues.\n",
        "#@markdown - **Equal Feature Importance**: If the scales of the features vary greatly, some ML algorithms may implicitly treat features with larger scales as more important. Normalization ensures all features have the same scale, so all features are treated equally (at least in terms of scale).\n",
        "\n",
        "#@markdown **Problem**: Here, we will want to normalize $K_d$ using the max-min normalization technique and then apply logarithm over binding affinities. (Apply the same technique for $K_i$ and $IC50$, individually).\n",
        "\n",
        "#@markdown The max-min normalization is the following: $$y_{norm} = \\frac{y - min(y)}{max(y) - min(y)}$$ where $y$ is the experimental measurements.\n",
        "\n",
        "#@markdown  note: when normalizing the test set, use min(y) and max(y) statistics from the training set.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "def normalize_values(values: np.single, min: float, max: float) -> list:\n",
        "  return (values - min) / (max-min)\n",
        "\n",
        "# normalize the training and test experimental measurements based on the training set statistics\n",
        "train_Kd_df['norm_Kd_values'] = normalize_values(\n",
        "    values=train_Kd_df['Kd_values[uM]'].values,\n",
        "    min=min(train_Kd_df['Kd_values[uM]'].values.tolist()),\n",
        "    max=max(train_Kd_df['Kd_values[uM]'].values.tolist())\n",
        "\n",
        ")\n",
        "# we want to take the logarithm of the normalized Kd measurements to reduce the dynamic range\n",
        "train_Kd_df['log_norm_Kd'] = np.log10(\n",
        "    train_Kd_df['norm_Kd_values'] + 1e-10\n",
        ")\n",
        "\n",
        "# Normalize the test set now...\n",
        "test_Kd_df['norm_Kd_values'] = normalize_values(\n",
        "    values=test_Kd_df['Kd_values[uM]'].values,\n",
        "    min=min(train_Kd_df['Kd_values[uM]'].values.tolist()), # important to note... use the training set statistics\n",
        "    max=max(train_Kd_df['Kd_values[uM]'].values.tolist())\n",
        "\n",
        "    )\n",
        "# we want to take the logarithm of the normalized Kd measurements to reduce the dynamic range\n",
        "test_Kd_df['log_norm_Kd'] = np.log10(\n",
        "    test_Kd_df['norm_Kd_values'] + 1e-10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siwQW-oFHN0j"
      },
      "source": [
        "#### Problem: can you explain why we are able to take the logarithm of $K_d$, $K_i$, or $IC50$ measurements? üö©\n",
        "\n",
        "note: think about the role of units (i.e. M, mM, uM etc).\n",
        "\n",
        "---\n",
        "\n",
        "**Answer:** $<$insert your explanation here$>$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wRGfT7YtGCoJ"
      },
      "outputs": [],
      "source": [
        "#@title Visualize the expected training/testing split distribution in terms of log( norm Kd).\n",
        "image_path = \"./UChicago_AIplusScience_code_workshops/figures/Kd_dataset_split.png\"\n",
        "display(Image(filename=image_path,width=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxDO30Bhyq90"
      },
      "outputs": [],
      "source": [
        "#@title Plot your normalized data and verify whether it matches the above figure.\n",
        "\n",
        "#@markdown **Problem: does your plot reproduce the above distributions for training and testing splits?** üö©\n",
        "\n",
        "\n",
        "# plot the log normalized kd measurements...\n",
        "plt.hist(train_Kd_df['log_norm_Kd'].values, bins=20, edgecolor='k', label='Train')\n",
        "plt.hist(test_Kd_df['log_norm_Kd'].values, bins=20, edgecolor='k', label='Test')\n",
        "plt.xlabel('log( norm Kd)')\n",
        "plt.ylabel('Number of protein-ligand complexes')\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ArJUVUD-ZYC"
      },
      "source": [
        "## Visualize molecule and protein structure üîé\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1J2lmYj2-lT3"
      },
      "outputs": [],
      "source": [
        "#@title Functions used for visualizing protein structures and ligand graphs üõ†Ô∏è\n",
        "def visualize_protein(pdb_path: str) -> None:\n",
        "  \"\"\"\n",
        "  function description: view the protein pdb structure in notebook\n",
        "  \"\"\"\n",
        "  protein_mdtraj = md.load_pdb(pdb_path) # protein PDB complex\n",
        "  # Get the topology object associated with the protein structure\n",
        "  topology = protein_mdtraj.topology\n",
        "\n",
        "  # Retrieve the protein sequence\n",
        "  protein_sequence = topology.to_fasta()[0]\n",
        "\n",
        "  view = nglview.show_mdtraj(protein_mdtraj)\n",
        "  print('Protein sequence:', protein_sequence)\n",
        "  display(view)  # interactive view outside Colab\n",
        "  return None\n",
        "\n",
        "def visualize_ligand(ligand_sdf_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Function description: View the molecular ligand using RDKit\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the SDF file\n",
        "    suppl = Chem.SDMolSupplier(ligand_sdf_path)\n",
        "\n",
        "    # Iterate over molecules in the SDF file\n",
        "    for mol in suppl:\n",
        "        if mol is not None:  # Check if molecule was read successfully\n",
        "            # Display the molecule\n",
        "            print(\"SMILEs ligand sequence:\", Chem.MolToSmiles(mol))\n",
        "            display(Draw.MolToImage(mol))\n",
        "        else:\n",
        "            # If the molecule was not read successfully, raise an error\n",
        "            raise ValueError(\"Unable to generate Molecular graph\")\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yHLcHl4Msgn"
      },
      "source": [
        "Cool examples of molecular graphs and protein structures are the following:\n",
        "  - `sample_index=50` and `train_set=True` # example from training set\n",
        "  - `sample_index=1423` and `train_set=True` # example from training set\n",
        "  - `sample_index=142` and `train_set=False` # example from test set\n",
        "  - `sample_index=100` and `train_set=False` # example from test set\n",
        "\n",
        "  Note: max `sample_index` for training and testing set is `2883` and `721`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HWWxczWuBICz"
      },
      "outputs": [],
      "source": [
        "#@title Visualize molcules üîé\n",
        "\n",
        "sample_index=1423#@param{type:\"integer\"}\n",
        "train_set=True#@param {type:\"boolean\"} # generate from the training\n",
        "\n",
        "if train_set:\n",
        "  visualize_ligand(ligand_sdf_path=train_Kd_df['ligand_sdf_paths'].loc[sample_index])\n",
        "  visualize_protein(pdb_path=train_Kd_df['protein_pdb_paths'].loc[sample_index])\n",
        "  print(f\"Binding affinity: {train_Kd_df['exp_measurement'].loc[sample_index]}\")\n",
        "else:\n",
        "  visualize_ligand(ligand_sdf_path=test_Kd_df['ligand_sdf_paths'].loc[sample_index])\n",
        "  visualize_protein(pdb_path=test_Kd_df['protein_pdb_paths'].loc[sample_index])\n",
        "  print(f\"Binding affinity: {test_Kd_df['exp_measurement'].loc[sample_index]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x90ic3IU6PG"
      },
      "source": [
        "# Problem: questions on molecules: proteins and molecular ligands. üö©\n",
        "\n",
        "1.) What are the four structural levels used to describe proteins?\n",
        "\n",
        "\n",
        "**Answer:** $<$insert your explanation here$>$\n",
        "\n",
        "---\n",
        "\n",
        "2.) Can you explain how the data processing differs between primary sequence structure and tertiary structure of a protein?\n",
        "\n",
        "**Answer:** $<$insert your explanation here$>$\n",
        "\n",
        "---\n",
        "\n",
        "3.) Could you list the key biochemical groups into which the 20 naturally occurring amino acids in proteins are divided?\n",
        "\n",
        "**Answer:** $<$insert your explanation here$>$\n",
        "\n",
        "---\n",
        "\n",
        "4.) How do hydrophobic amino acids contribute to the formation of a protein's tertiary structure?\n",
        "\n",
        "**Answer:** $<$insert your explanation here$>$\n",
        "\n",
        "---\n",
        "\n",
        "5.) How does the chemical nature of a molecular ligand affect its interaction with protein targets?\n",
        "\n",
        "**Answer:** $<$insert your explanation here$>$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "6.) How does the structural complexity of a ligand influence its biological activity?\n",
        "\n",
        "**Answer:** $<$insert your explanation here$>$\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7vtUiVLbwgp"
      },
      "source": [
        "# Discriminative task üí™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmPs_zIJnjY"
      },
      "source": [
        "# Ligand sequence-based encoder model\n",
        "\n",
        "Here, we will train a single encoder model (i.e. mutli-layer perceptron) model which takes in inputs of ligand sequences as SMILES strings and outputs continous values that corresponding the log normalized dissociation constants log($\\tilde{K_d}$). The variable $\\tilde{K_d}$ corresponds to the normalized $K_d$ using max-min normalization. To save time during the workshop, we can skip training the model from scratch and load weights from a previously trained configuration. #TODO: insert illustration of the ligand sequence-based encoder.\n",
        "\n",
        "* Note: we will load the pretrained protein sequence-based encoder model, but we will train the dual ligand- and protein- sequence based encoders and compare test metric performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "upXmSZjDJ3fe"
      },
      "outputs": [],
      "source": [
        "#@title Illustration of the Ligand-based sequence only encoder architecture.\n",
        "\n",
        "#@markdown We will process the input data as SMILES string data and use a simple MLP archiecture to predict real-value regression values $y$ (i.e. log( norm Kd)).\n",
        "\n",
        "image_path = \"./UChicago_AIplusScience_code_workshops/figures/Ligand_sequence_encoder.png\"\n",
        "display(Image(filename=image_path,width=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KnyaVaGdnUa"
      },
      "outputs": [],
      "source": [
        "##@markdown `ligand_smiles_dataset` is needed for process a batch of ligand SMILES strings for training a neural network model in pytorch.\n",
        "\n",
        "#@markdown Note: For now, we will share a complete Dataset class to implement on the Ligand sequence-based encoder as a reference. It will be helpful when creating your Dataset class when developing and training dual encoder model for processing protein and ligand sequences.\n",
        "\n",
        "#@markdown Second note: `__getitem__()` processes the data for a given batch during training. Please study the `smiles_tokenizer()` and `__getitem__()` hook for how to process batch data in PyTorch and how to convert SMILES strings into tensors.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "class ligand_smiles_dataset(Dataset):\n",
        "\n",
        "  def __init__(self, data: list, output_labels: list, max_seq_len: int):\n",
        "\n",
        "    self.smiles_data = data # input smiles sequences\n",
        "    self.output_labels = output_labels # binding affinity (Kd measurements)\n",
        "    self.max_seq_len = max_seq_len # max sequence length\n",
        "    self.tokenizer = dc.feat.OneHotFeaturizer() # ligand SMILEs tokenization\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.smiles_data)\n",
        "\n",
        "  def smiles_tokenizer(self, smiles_sample):\n",
        "\n",
        "    # convert string characters to one hot encoded tensors\n",
        "    features =  torch.Tensor(self.tokenizer.featurize(smiles_sample)[:,0,:])\n",
        "\n",
        "    # compute the padding length\n",
        "    pad_len = self.max_seq_len - features.size(0)\n",
        "    # create a tensor of zeros of shape (pad_len, 35)\n",
        "    pad_tensor = torch.zeros(pad_len, 35)\n",
        "    # set the last dimension of each vector in pad tensor to 1 (i.e. padded class)\n",
        "    pad_tensor[:, -1] = 1\n",
        "    # concatenate features and pad_tensor along dimension 0\n",
        "    x_padded = torch.cat((features, pad_tensor), dim=0)\n",
        "\n",
        "    return x_padded\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # prepare input data\n",
        "    smiles = self.smiles_data[idx]\n",
        "    x_ligands = self.smiles_tokenizer(smiles_sample=smiles)\n",
        "\n",
        "    # prepare output/prediction data\n",
        "    y_true = torch.tensor(self.output_labels[idx])\n",
        "\n",
        "    return (\n",
        "        x_ligands,\n",
        "        y_true\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZQBcF90idw1"
      },
      "outputs": [],
      "source": [
        "#@title Prepare the Pytorch dataset and dataloader for training\n",
        "\n",
        "' set up ligand training dataset with Dataset and Dataloader using pytorch'\n",
        "train_ligand_dataset = ligand_smiles_dataset(\n",
        "    data=train_Kd_df['ligand_smiles'].tolist(),\n",
        "    output_labels=train_Kd_df['log_norm_Kd'].tolist(),\n",
        "    max_seq_len=max(len(s) for s in train_Kd_df['ligand_smiles'].tolist())\n",
        ")\n",
        "train_ligand_dataloader = DataLoader(\n",
        "        train_ligand_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=True,# shuffle is only 'True' for training sets.\n",
        "        num_workers=0\n",
        ")\n",
        "' set up ligand testing dataset with Dataset and Dataloader using pytorch'\n",
        "\n",
        "test_ligand_dataset = ligand_smiles_dataset(\n",
        "    data=test_Kd_df['ligand_smiles'].tolist(),\n",
        "    output_labels=test_Kd_df['log_norm_Kd'].tolist(),\n",
        "    max_seq_len=max(len(s) for s in train_Kd_df['ligand_smiles'].tolist())\n",
        ")\n",
        "test_ligand_dataloader = DataLoader(\n",
        "        test_ligand_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=False, # shuffle is only 'True' for training sets.\n",
        "        num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmLYaDfSjOGG"
      },
      "outputs": [],
      "source": [
        "#@title Iterate over an batch example with the dataloader.\n",
        "\n",
        "#@markdown Patience: One pass through the dataloader for the Ligand might take some time since it requires tokenizing SMILES strings.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "' here we will run a single pass through the data loader as an example '\n",
        "train_x_ligands, train_y_true = next(iter(train_ligand_dataloader))\n",
        "print('Training set: Shape of the ligand smile input and output labels:', list(train_x_ligands.shape), list(train_y_true.shape))\n",
        "\n",
        "' here we will run a single pass through the data loader as an example '\n",
        "test_x_ligands, test_y_true = next(iter(test_ligand_dataloader))\n",
        "print('Testing set: Shape of the ligand smile input and output labels:', list(test_x_ligands.shape), list(test_y_true.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGa2882fmYhb"
      },
      "outputs": [],
      "source": [
        "#@title Simple MLP for regression\n",
        "\n",
        "#@markdown `simple_MLP` is a neural network function that processes Ligand SMILES strings and predicts log $\\tilde{K_d}$ values, i.e., $y$~ $p_{\\theta}(y|x_l)$.\n",
        "\n",
        "#@markdown Note: This network is agnostic to input data, and thus can be used for either Ligand SMILES data or Protein sequence data, assuming that architecture hyperparameters are changed accordingly.\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "class simple_MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size: int, hidden_size: int, depth: int):\n",
        "\n",
        "    super(simple_MLP, self).__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    # input layers\n",
        "    self.layers.append(nn.Linear(input_size, hidden_size))\n",
        "    self.layers.append(nn.GELU())\n",
        "\n",
        "    # hidden layers\n",
        "    for _ in range(depth - 1):\n",
        "\n",
        "      self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "      self.layers.append(nn.GELU())\n",
        "\n",
        "    # output layer\n",
        "    self.layers.append(nn.Linear(hidden_size, 1))\n",
        "\n",
        "  # forward pass of neural network y ~ f(x)\n",
        "  def forward(self, x):\n",
        "    # Flatten the input tensor\n",
        "    x = x.view(x.size(0), -1)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    y_pred = x.clone() # for pedagogical purposes..\n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "445IRASCnk2s"
      },
      "outputs": [],
      "source": [
        "#@title Define the ligand model hyperparameters for affinity prediction\n",
        "\n",
        "#@markdown Note: the hyperparameters are given here.\n",
        "#@markdown ---\n",
        "\n",
        "seq_len = train_x_ligands.shape[1] # sequence length\n",
        "char_tokens = train_x_ligands.shape[-1] # number of smiles character tokens\n",
        "input_size=seq_len * char_tokens # input dimension to MLP\n",
        "hidden_size=500 # hyperparameter; control expressive the neural network is...\n",
        "depth=2 # hyperparameter; control how deep the neural network is...\n",
        "\n",
        "ligand_model = simple_MLP(input_size=input_size, hidden_size=hidden_size, depth=depth)\n",
        "ligand_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U4X-olYap_yo"
      },
      "outputs": [],
      "source": [
        "#@title Function for tracking performance metrics üõ†Ô∏è\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "@torch.no_grad()\n",
        "def performance_metrics(y_true, y_pred):\n",
        "\n",
        "    if len(y_true.shape) == 2: # if the shape of tensor is (batch_size, 1), squash the last dim. output: (batch_size,)\n",
        "      y_true.squeeze(-1)\n",
        "\n",
        "    if len(y_pred.shape) == 2: # if the shape of tensor is (batch_size, 1), squash the last dim. output: (batch_size,)\n",
        "      y_pred.squeeze(-1)\n",
        "\n",
        "    # Move tensors to CPU and convert to numpy\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "    # Compute Pearson correlation\n",
        "    pearson_corr, _ = pearsonr(y_true, y_pred)\n",
        "\n",
        "    # Compute Spearman correlation\n",
        "    spearman_corr, _ = spearmanr(y_true, y_pred)\n",
        "\n",
        "    return pearson_corr, spearman_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oLlLb38V4LaN"
      },
      "outputs": [],
      "source": [
        "#@title Function for compute metrics on test set üõ†Ô∏è\n",
        "@torch.no_grad()\n",
        "def test_performance_metrics(model: nn.Module, dataloader: DataLoader):\n",
        "\n",
        "  model.eval()\n",
        "  # Define a loss function\n",
        "  criterion = nn.MSELoss()  # For simple regression prediction, we can use mean_squared_error loss.\n",
        "\n",
        "  y_true_batch, y_pred_batch = [], []\n",
        "\n",
        "  for i, (x, y_true) in enumerate(dataloader):\n",
        "\n",
        "    # Move inputs and labels to the device\n",
        "    x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "    # Forward propagation\n",
        "    y_pred = model(x).squeeze(1)\n",
        "\n",
        "    y_true_batch.append(y_true)\n",
        "    y_pred_batch.append(y_pred)\n",
        "\n",
        "  # concatenate batches\n",
        "  y_true_batch = torch.cat(y_true_batch, dim=0)\n",
        "  y_pred_batch = torch.cat(y_pred_batch, dim=0)\n",
        "\n",
        "  # Loss computation\n",
        "  loss = criterion(y_pred_batch, y_true_batch)\n",
        "\n",
        "  # get metric performance\n",
        "  pearson_corr, spearman_rho = performance_metrics(\n",
        "                y_true=y_true_batch,\n",
        "                y_pred=y_pred_batch\n",
        "  )\n",
        "  print(f\"Test metrics | Loss: {loss.item():.3f}, Pearson: {pearson_corr.item():.3f}, Spearman rho: {spearman_rho.item():.3f}\")\n",
        "\n",
        "  return (\n",
        "      loss.item(),\n",
        "      pearson_corr.item(),\n",
        "      spearman_rho.item()\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu0O2nusoDQA"
      },
      "outputs": [],
      "source": [
        "#@title Train the Ligand sequence-based model üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
        "\n",
        "# Define a loss function\n",
        "criterion = nn.MSELoss()  # For simple regression prediction, we can use mean_squared_error loss.\n",
        "\n",
        "# define an optimizer\n",
        "optimizer = optim.SGD(ligand_model.parameters(), lr=0.01)  # lr corresponds to learning_rate (keep fixed for now)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ligand_model.to(device)\n",
        "\n",
        "# number of training cycles over the dataset (i.e. epochs)\n",
        "num_epochs = 1\n",
        "\n",
        "# track losses and metrics\n",
        "train_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "test_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "iteration = 0\n",
        "# Start the training loop\n",
        "for epoch in range(num_epochs):  # num_epochs is the number of epochs to train for\n",
        "    for i, (x_ligand, y_true) in enumerate(train_ligand_dataloader ):\n",
        "        # Move inputs and labels to the device\n",
        "        x_ligand, y_true = x_ligand.to(device), y_true.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        y_pred = ligand_model(x_ligand).squeeze(1)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "        pearson_corr, spearman_rho = performance_metrics(\n",
        "                y_true=y_true,\n",
        "                y_pred=y_pred\n",
        "        )\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_ligand_dataloader)}, Loss: {loss.item():.3f}, Pearson: {pearson_corr.item():.3f}, Spearman rho: {spearman_rho.item():.3f}\")\n",
        "\n",
        "\n",
        "        # log training performance\n",
        "        train_history_dict['step'].append(iteration)\n",
        "        iteration+=1\n",
        "        train_history_dict['loss'].append(loss.item())\n",
        "        train_history_dict['pearson_corr'].append(pearson_corr.item())\n",
        "        train_history_dict['spearman_rho'].append(spearman_rho.item())\n",
        "\n",
        "    # compute performance metrics on the test set\n",
        "    test_loss, test_pearson_corr, test_spearman_rho = test_performance_metrics(model=model, dataloader=test_ligand_dataloader)\n",
        "    test_history_dict['step'].append(iteration)\n",
        "    test_history_dict['loss'].append(test_loss)\n",
        "    test_history_dict['pearson_corr'].append(test_pearson_corr)\n",
        "    test_history_dict['spearman_rho'].append(test_spearman_rho)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy4IKXvzDP4s"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training history üîé\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10,3))\n",
        "\n",
        "axes[0].plot(train_history_dict['step'], train_history_dict['loss'])\n",
        "axes[0].plot(test_history_dict['step'], test_history_dict['loss'], '.-', markersize=10)\n",
        "axes[0].set_ylabel('loss')\n",
        "axes[0].set_xlabel('steps')\n",
        "\n",
        "axes[1].plot(train_history_dict['step'], train_history_dict['pearson_corr'])\n",
        "axes[1].plot(test_history_dict['step'], test_history_dict['pearson_corr'], '.-', markersize=10)\n",
        "axes[1].set_ylabel('loss')\n",
        "axes[1].set_xlabel('steps')\n",
        "\n",
        "axes[2].plot(train_history_dict['step'], train_history_dict['spearman_rho'])\n",
        "axes[2].plot(test_history_dict['step'], test_history_dict['spearman_rho'], '.-', markersize=10)\n",
        "axes[2].set_ylabel('loss')\n",
        "axes[2].set_xlabel('steps')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gilIB1JjooKj"
      },
      "outputs": [],
      "source": [
        "#@title #### Problem: can you use the loaded pretrain model to predict $log( norm(K_d))$ and compare against ground truth? üö©\n",
        "\n",
        "#@markdown Note: you can compute statistics like pearson correlation or spearmann $\\rho$ correlation too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjOzjHgiJ6WB"
      },
      "source": [
        "# Protein sequence-based encoder model\n",
        "\n",
        "We will follow a similar protocol as the `Ligand sequence-based encoder model`; however, we will slightly have to develop a different $dataset$ $iterator$ since protein preprocessing requires a different approach than ligand preprocessing. We can still use the same MLP architecture, but we will require a different input dimension to accoutn for the protein sequence dataset. The steps that this section will inform us is the following:\n",
        "\n",
        "  - Create dataset iterator and dataloader for processing protein sequences in batch.\n",
        "  - Create the model architecture for binding affinity prediction.\n",
        "  - Create the boiler plate for training the model on data. `Note:` To save time for training dual encoder model, we will load pretrained weights and corresponding results on test set.  \n",
        "  - Post-process results and compare performance ligand encoder model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r58LsWf7Oq4N"
      },
      "outputs": [],
      "source": [
        "#@title Illustration of the Protein-based sequence only encoder architecture.\n",
        "\n",
        "#@markdown We will process the input data as Protein amino acid sequence data and use a simple MLP architecture to predict real-value regression values $y$ (i.e. log( norm Kd)).\n",
        "\n",
        "image_path = \"./UChicago_AIplusScience_code_workshops/figures/Protein_sequence_encoder.png\"\n",
        "display(Image(filename=image_path,width=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sya0_XUbJ5Yc"
      },
      "outputs": [],
      "source": [
        "#@markdown `protein_sequences_dataset` is needed for process a batch of protein sequences  for training a neural network model in pytorch.\n",
        "\n",
        "#@markdown Note: For now, we will share a complete Dataset class to implement on the protein sequence-based encoder as a reference. It will be helpful when creating your Dataset class when developing and training dual encoder model for processing protein and ligand sequences.\n",
        "\n",
        "\n",
        "#@markdown Second note: `__getitem__()` processes the data for a given batch during training.\n",
        "\n",
        "#@markdown Third note: `protein_tokenizer()` converts the amino acid characters into a tensor which can be processed by machine learning algorithms. Please keep track of the `__getitem__()` and `protein_tokenizer()` as it will help with building a dataset iterator when we need to process protein-ligand data in parallel.\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "class protein_sequences_dataset(Dataset):\n",
        "\n",
        "  def __init__(self, data: list, output_labels: list, max_seq_len: int):\n",
        "\n",
        "    self.protein_data = data # input smiles sequences\n",
        "    self.output_labels = output_labels # binding affinity (Kd measurements)\n",
        "    self.max_seq_len = max_seq_len # max sequence length\n",
        "    # Define the tokens to cover all the possible amino acids including special ones\n",
        "    self.tokens = [\n",
        "        'A', 'C', 'D', 'E', 'F', 'G', 'H',\n",
        "        'I', 'K', 'L', 'M', 'N', 'P', 'Q',\n",
        "        'R', 'S', 'T', 'V', 'W', 'Y', 'U',\n",
        "        'O', '-'\n",
        "    ]\n",
        "\n",
        "    # setup the amino acid tokenizer\n",
        "    self.tokenizer = {token:ii for ii, token in enumerate(self.tokens)}\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.protein_data)\n",
        "\n",
        "\n",
        "  def protein_tokenizer(self, protein_sample):\n",
        "      # convert amino acid characters into numerical values\n",
        "      num_protein_sample = [self.tokenizer[aa_char] for aa_char in protein_sample]\n",
        "      # one hot encoding\n",
        "      one_hot = F.one_hot(torch.tensor(num_protein_sample), num_classes=len(self.tokens))\n",
        "      # compute the padding length\n",
        "      pad_len = self.max_seq_len - one_hot.size(0)\n",
        "      # create a tensor of zeros of shape (pad_len, len(self.tokens))\n",
        "      pad_tensor = torch.zeros(pad_len, len(self.tokens)).long()\n",
        "      # set the padding tensor with the 'PAD' index\n",
        "      pad_tensor[:, self.tokenizer['-']] = 1\n",
        "      # concatenate features and pad_tensor along dimension 0\n",
        "      x_padded = torch.cat((one_hot, pad_tensor), dim=0)\n",
        "      return x_padded\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # prepare input data\n",
        "    protein_sample = self.protein_data[idx]\n",
        "    x_proteins = self.protein_tokenizer(protein_sample=protein_sample)\n",
        "\n",
        "    # prepare output/prediction data\n",
        "    y_true = torch.tensor(self.output_labels[idx])\n",
        "\n",
        "    return (\n",
        "        x_proteins.float(),\n",
        "        y_true\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzL60NAgP_Gs"
      },
      "outputs": [],
      "source": [
        "#@title Create and define Pytorch dataset iterator and dataloader\n",
        "\n",
        "\n",
        "' set up protein training dataset with Dataset and Dataloader using pytorch'\n",
        "train_protein_dataset = protein_sequences_dataset(\n",
        "    data=train_Kd_df['protein_sequence'].tolist(),\n",
        "    output_labels=train_Kd_df['log_norm_Kd'].tolist(),\n",
        "    max_seq_len=max(len(s) for s in train_Kd_df['protein_sequence'].tolist())\n",
        ")\n",
        "train_protein_dataloader = DataLoader(\n",
        "        train_protein_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=True,# shuffle is only 'True' for training sets.\n",
        "        num_workers=4\n",
        ")\n",
        "' set up protein testing dataset with Dataset and Dataloader using pytorch'\n",
        "\n",
        "test_protein_dataset = protein_sequences_dataset(\n",
        "    data=test_Kd_df['protein_sequence'].tolist(),\n",
        "    output_labels=test_Kd_df['log_norm_Kd'].tolist(),\n",
        "    max_seq_len=max(len(s) for s in train_Kd_df['protein_sequence'].tolist())\n",
        ")\n",
        "test_protein_dataloader = DataLoader(\n",
        "        test_protein_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=False, # shuffle is only 'True' for training sets.\n",
        "        num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTiSkkCeQYCM"
      },
      "outputs": [],
      "source": [
        "' here we will run a single pass through the data loader as an example '\n",
        "train_x_proteins, train_y_true = next(iter(train_protein_dataloader))\n",
        "print('Training set: Shape of the protein sequence input and output labels:', list(train_x_proteins.shape), list(train_y_true.shape))\n",
        "\n",
        "' here we will run a single pass through the data loader as an example '\n",
        "test_x_proteins, test_y_true = next(iter(test_protein_dataloader))\n",
        "print('Testing set: Shape of the protein sequence input and output labels:', list(test_x_proteins.shape), list(test_y_true.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2zfxLa8SIwh"
      },
      "outputs": [],
      "source": [
        "#@title Define the protein sequence-based model for affinity prediction\n",
        "\n",
        "protein_seq_len = train_x_proteins.shape[1] # sequence length\n",
        "protein_char_tokens = train_x_proteins.shape[-1] # number of protein sequence tokens\n",
        "protein_input_size=protein_seq_len * protein_char_tokens # input dimension to MLP\n",
        "protein_hidden_size=50 # hyperparameter; control expressive the neural network is...\n",
        "protein_depth=1 # hyperparameter; control how deep the neural network is...\n",
        "\n",
        "protein_model = simple_MLP(input_size=protein_input_size, hidden_size=protein_hidden_size, depth=protein_depth)\n",
        "protein_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsrzFZe_SewL"
      },
      "source": [
        "## Note for training protein sequence-based encoder.\n",
        "\n",
        "We can use the same training boiler plate, performance metric functions, and testing prediction functions as the ligand sequence-based encoder since our loss objective is the same. However, we will need to use our new dataloaders and model architecture for the protein dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q02SwtkBScpC"
      },
      "outputs": [],
      "source": [
        "#@title Train the Protein sequence-based model üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
        "\n",
        "#@markdown Here, since the training is quite fast for protein sequences unlike ligand SMILES strings, we will train the model from scratch.\n",
        "#@markdown ---\n",
        "\n",
        "# Define a loss function\n",
        "criterion = nn.MSELoss()  # For simple regression prediction, we can use mean_squared_error loss.\n",
        "\n",
        "# define an optimizer\n",
        "optimizer = optim.SGD(protein_model.parameters(), lr=0.01)  # lr corresponds to learning_rate (keep fixed for now)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = protein_model.to(device)\n",
        "\n",
        "#@markdown Decide whether to\n",
        "train_model_option = 'train' #@param ['train', 'load_weights']{type:\"string\"}\n",
        "\n",
        "# number of training cycles over the dataset (i.e. epochs)\n",
        "num_epochs = 30 #@param [0, 1, 10, 15, 30] {type:\"raw\"}\n",
        "\n",
        "\n",
        "# track losses and metrics\n",
        "train_protein_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "test_protein_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "iteration = 0\n",
        "# Start the training loop\n",
        "for epoch in range(num_epochs):  # num_epochs is the number of epochs to train for\n",
        "    for i, (x_protein, y_true) in enumerate(train_protein_dataloader ):\n",
        "        # Move inputs and labels to the device\n",
        "        x_protein, y_true = x_protein.to(device), y_true.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        y_pred = protein_model(x_protein).squeeze(1)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "        pearson_corr, spearman_rho = performance_metrics(\n",
        "                y_true=y_true,\n",
        "                y_pred=y_pred\n",
        "        )\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_protein_dataloader)}, Loss: {loss.item():.3f}, Pearson: {pearson_corr.item():.3f}, Spearman rho: {spearman_rho.item():.3f}\")\n",
        "\n",
        "\n",
        "        # log training performance\n",
        "        train_protein_history_dict['step'].append(iteration)\n",
        "        iteration+=1\n",
        "        train_protein_history_dict['loss'].append(loss.item())\n",
        "        train_protein_history_dict['pearson_corr'].append(pearson_corr.item())\n",
        "        train_protein_history_dict['spearman_rho'].append(spearman_rho.item())\n",
        "\n",
        "    # compute performance metrics on the test set\n",
        "    test_loss, test_pearson_corr, test_spearman_rho = test_performance_metrics(model=protein_model, dataloader=test_protein_dataloader)\n",
        "    test_protein_history_dict['step'].append(iteration)\n",
        "    test_protein_history_dict['loss'].append(test_loss)\n",
        "    test_protein_history_dict['pearson_corr'].append(test_pearson_corr)\n",
        "    test_protein_history_dict['spearman_rho'].append(test_spearman_rho)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFH4OMJhXHin"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training history üîé\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10,3))\n",
        "\n",
        "axes[0].plot(train_protein_history_dict['step'], train_protein_history_dict['loss'])\n",
        "axes[0].plot(test_protein_history_dict['step'], test_protein_history_dict['loss'], '.-', markersize=10)\n",
        "axes[0].set_ylabel('loss')\n",
        "axes[0].set_xlabel('steps')\n",
        "\n",
        "axes[1].plot(train_protein_history_dict['step'], train_protein_history_dict['pearson_corr'])\n",
        "axes[1].plot(test_protein_history_dict['step'], test_protein_history_dict['pearson_corr'], '.-', markersize=10)\n",
        "axes[1].set_ylabel('Pearson corr.')\n",
        "axes[1].set_xlabel('steps')\n",
        "\n",
        "axes[2].plot(train_protein_history_dict['step'], train_protein_history_dict['spearman_rho'])\n",
        "axes[2].plot(test_protein_history_dict['step'], test_protein_history_dict['spearman_rho'], '.-', markersize=10)\n",
        "axes[2].set_ylabel('Spearman rho')\n",
        "axes[2].set_xlabel('steps')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6z3sj8cRoXN"
      },
      "source": [
        "#### Summarize outputs:\n",
        "\n",
        "We can see that the max sequence length for the protein sequence dataset is 1179, which is drastically longer than the SMILES strings. However, the number of class labels for representing the protein sequence (23) is lower than the SMILES string (35)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J_IoAKoU-5d"
      },
      "source": [
        "# Protein and Ligand sequence-based encoders model\n",
        "\n",
        "The advantage of deep learning is the flexibility of handling multiple data types in one individual architecture. In other words, a deep learning architecture allows us to either take multiple inputs (e.g. protein sequence and ligand SMILES string) and output a single prediction (e.g. binding affinity) -- and vice-versa. This is usually referred to mutlimodal deep learning, and some resources in this category are the following:\n",
        "\n",
        "  - short blog post: https://serokell.io/blog/multimodal-machine-learning\n",
        "  - slides with many examples: https://web.stanford.edu/class/cs224n/slides/Multimodal-Deep-Learning-CS224n-Kiela.pdf\n",
        "  - lecture for multimodal deep learning in protein engineering: https://www.youtube.com/watch?v=qFSVVWcCRHs\n",
        "  - survey paper: https://dl.acm.org/doi/10.1109/TPAMI.2018.2798607\n",
        "  \n",
        "---\n",
        "**Why is multimodal learning important for protein-ligand binding prediction?**\n",
        "\n",
        "For starters, we have been predicting binding affinities using a neural network when it only has access to either protein sequence data or ligand data. This is of course is quite naive considering that binding affinites correspond to interaction between the protein sequence and ligand molecular and is not a intrinsic properties of the individual molecules. Thus, it makes more sense to develop a neural network model that predicts binding affinities given knowledge of the protein sequence and corresponding ligand graph. In addition, the preprocessing and representation of biomolcules like proteins and small molecular graphs like ligands are processed differently, and thus require a \"multi-modal\" approach.\n",
        "\n",
        "üî• **Goal** of this section is to develop a dataloader and train a neural network from scratch, which processes input data of both protein sequences and ligand SMILES strings and predicts regression value for $log \\tilde{K_d}$. The architecture of this dual encoder is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ha8ketBt21ev"
      },
      "outputs": [],
      "source": [
        "#@title Illustration of the Protein+Ligand-based sequence only encoder architecture.\n",
        "\n",
        "#@markdown We will process the input data as Protein amino acid sequence data and use a simple MLP archiecture to predict real-value regression values $y$ (i.e. log( norm Kd)).\n",
        "\n",
        "image_path = \"./UChicago_AIplusScience_code_workshops/figures/Protein_Ligand_sequence_encoders.png\"\n",
        "display(Image(filename=image_path,width=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "41ZJ91WSl3Cp"
      },
      "outputs": [],
      "source": [
        "#@title Remove cache and save memory.\n",
        "import gc\n",
        "# After deleting the DataLoader\n",
        "del train_protein_dataloader, train_ligand_dataloader, test_protein_dataloader, test_ligand_dataloader\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jbWq8_uU5Fg"
      },
      "outputs": [],
      "source": [
        "#@title Problem: create a dataloader that processes both Protein sequences and Ligand SMILES strings. üö©\n",
        "\n",
        "#@markdown Coding task 1 üö©: how would you fill in the following hook `preprocess_protein()` for processing the protein sequences? Recall: the input to this hook is strings containing amino acid characters, while the output should correspond to a padded torch tensor.\n",
        "\n",
        "#@markdown Coding task 2 üö©: how would you fill in the following hook `preprocess_smiles()` for processing the ligand smiles? Recall: the input to this hook is strings containing SMILES characters, while the output should correspond to a padded torch tensor.\n",
        "\n",
        "#@markdown Coding task 3 üö©: how would you fill in the `__getitem__()` which creates the batch samples to train your model? I left comments within the hook to indicate the three main steps to process protein and ligand sequence data in parallel: - (1) \"preprocess protein data\", \"preprocess ligand data\", and \"preprocess output label data\"\n",
        "\n",
        "\n",
        "#@markdown note: we can use the dataloaders `protein_sequences_dataset` and `ligand_smiles_dataset` as a reference.\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "class protein_ligand_sequence_dataset(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      protein_data: list,\n",
        "      protein_max_seq_len: int,\n",
        "      ligand_data: list,\n",
        "      ligand_max_seq_len: int,\n",
        "      output_labels: list,\n",
        "      ):\n",
        "\n",
        "    # binding affinity measurements\n",
        "    self.output_labels = output_labels\n",
        "\n",
        "    ' preprocess protein data'\n",
        "    self.protein_data = protein_data # input protein sequences\n",
        "    self.protein_max_seq_len = protein_max_seq_len # max sequence length\n",
        "    # Define the tokens to cover all the possible amino acids including special ones\n",
        "    self.protein_tokens = [\n",
        "        'A', 'C', 'D', 'E', 'F', 'G', 'H',\n",
        "        'I', 'K', 'L', 'M', 'N', 'P', 'Q',\n",
        "        'R', 'S', 'T', 'V', 'W', 'Y', 'U',\n",
        "        'O', '-'\n",
        "    ]\n",
        "    # setup the amino acid tokenizer\n",
        "    self.protein_tokenizer = {token:ii for ii, token in enumerate(self.protein_tokens)}\n",
        "\n",
        "    ' preprocess ligand data'\n",
        "    self.ligand_data = ligand_data # input ligand SMILES sequences\n",
        "    self.ligand_max_seq_len = ligand_max_seq_len # max sequence length\n",
        "    # ligand SMILEs tokenization\n",
        "    self.ligand_tokenizer = dc.feat.OneHotFeaturizer() # ligand SMILEs tokenization\n",
        "\n",
        "  def __len__(self):\n",
        "    assert len(self.protein_data) == len(self.ligand_data), \"Length of protein_data and ligand_data should be equal\"\n",
        "    return len(self.protein_data)\n",
        "\n",
        "  def preprocess_protein(self, protein_sample):\n",
        "      # convert amino acid characters into numerical values\n",
        "      num_protein_sample = [self.protein_tokenizer[aa_char] for aa_char in protein_sample]\n",
        "      # one hot encoding\n",
        "      one_hot = F.one_hot(torch.tensor(num_protein_sample), num_classes=len(self.protein_tokens))\n",
        "      # compute the padding length\n",
        "      pad_len = self.protein_max_seq_len - one_hot.size(0)\n",
        "      # create a tensor of zeros of shape (pad_len, len(self.tokens))\n",
        "      pad_tensor = torch.zeros(pad_len, len(self.protein_tokens)).long()\n",
        "      # set the padding tensor with the 'PAD' index\n",
        "      pad_tensor[:, self.protein_tokenizer['-']] = 1\n",
        "      # concatenate features and pad_tensor along dimension 0\n",
        "      x_padded = torch.cat((one_hot, pad_tensor), dim=0)\n",
        "      return x_padded\n",
        "\n",
        "  def preprocess_smiles(self, smiles_sample):\n",
        "\n",
        "    # convert string characters to one hot encoded tensors\n",
        "    features =  torch.Tensor(self.ligand_tokenizer.featurize(smiles_sample)[:,0,:])\n",
        "    # compute the padding length\n",
        "    pad_len = self.ligand_max_seq_len - features.size(0)\n",
        "    # create a tensor of zeros of shape (pad_len, 35)\n",
        "    pad_tensor = torch.zeros(pad_len, 35)\n",
        "    # set the last dimension of each vector in pad tensor to 1 (i.e. padded class)\n",
        "    pad_tensor[:, -1] = 1\n",
        "    # concatenate features and pad_tensor along dimension 0\n",
        "    x_padded = torch.cat((features, pad_tensor), dim=0)\n",
        "    return x_padded\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    ' preprocess protein data'\n",
        "    # prepare input data\n",
        "    protein_sample = self.protein_data[idx]\n",
        "    x_proteins = self.preprocess_protein(protein_sample=protein_sample)\n",
        "\n",
        "    ' preprocess ligand data'\n",
        "    # prepare input data\n",
        "    ligands = self.ligand_data[idx]\n",
        "    x_ligands = self.preprocess_smiles(smiles_sample=ligands)\n",
        "\n",
        "    ' preprocess output label data'\n",
        "    # prepare output/prediction data\n",
        "    y_true = torch.tensor(self.output_labels[idx])\n",
        "\n",
        "    return (\n",
        "        x_proteins.float(),\n",
        "        x_ligands.float(),\n",
        "        y_true\n",
        "    )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it3CvIcxQZn-"
      },
      "outputs": [],
      "source": [
        "' set up protein and ligand training dataset with Dataset and Dataloader using pytorch'\n",
        "train_protein_ligand_dataset = protein_ligand_sequence_dataset(\n",
        "    protein_data=train_Kd_df['protein_sequence'].tolist(),\n",
        "    protein_max_seq_len=max(len(s) for s in train_Kd_df['protein_sequence'].tolist()),\n",
        "    ligand_data=train_Kd_df['ligand_smiles'].tolist(),\n",
        "    ligand_max_seq_len=max(len(s) for s in train_Kd_df['ligand_smiles'].tolist()),\n",
        "    output_labels=train_Kd_df['log_norm_Kd'].tolist(),\n",
        "\n",
        ")\n",
        "train_protein_ligand_dataloader = DataLoader(\n",
        "        train_protein_ligand_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=True,# shuffle is only 'True' for training sets.\n",
        "        num_workers=0\n",
        ")\n",
        "' set up protein and ligand testing dataset with Dataset and Dataloader using pytorch'\n",
        "\n",
        "test_protein_ligand_dataset = protein_ligand_sequence_dataset(\n",
        "    protein_data=test_Kd_df['protein_sequence'].tolist(),\n",
        "    protein_max_seq_len=max(len(s) for s in train_Kd_df['protein_sequence'].tolist()),\n",
        "    ligand_data=test_Kd_df['ligand_smiles'].tolist(),\n",
        "    ligand_max_seq_len=max(len(s) for s in train_Kd_df['ligand_smiles'].tolist()),\n",
        "    output_labels=test_Kd_df['log_norm_Kd'].tolist(),\n",
        ")\n",
        "test_protein_ligand_dataloader = DataLoader(\n",
        "        test_protein_ligand_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=False, # shuffle is only 'True' for training sets.\n",
        "        num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlUXElyyb0YF"
      },
      "outputs": [],
      "source": [
        "' here we will run a single pass through the data loader as an example '\n",
        "train_x_proteins, train_x_ligands, train_y_true = next(iter(train_protein_ligand_dataloader))\n",
        "print('Training set: Shape of the protein sequence, ligand SMILES input and output labels:', train_x_proteins.shape, train_x_ligands.shape, train_y_true.shape)\n",
        "\n",
        "' here we will run a single pass through the data loader as an example '\n",
        "test_x_proteins, test_x_ligands, test_y_true = next(iter(test_protein_ligand_dataloader))\n",
        "print('Testing set: Shape of the protein sequence, ligand SMILES input and output labels:', test_x_proteins.shape, test_x_ligands.shape, test_y_true.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Fepg0Bk_uS"
      },
      "outputs": [],
      "source": [
        "#@title Problem: create a dual encoder MLP that each processes protein and ligand data üö©\n",
        "\n",
        "#@markdown `dual_encoder_MLP` is a neural network function that processes both Protein sequences and Ligand SMILES strings and predicts log $\\tilde{K_d}$ values. The architecture is illustrated with the following figure.\n",
        "\n",
        "\n",
        "#@markdown `Note:` Create a similar architecture as `simple_MLP` but have two encoders where each one processes either protein or ligand data.\n",
        "\n",
        "#@markdown `Second Note:` Each encoder will convert the protein or ligand sequence into a hidden representation $h_s$ or $h_t$. We will concatenate these representations into $h=[h_s,h_t]$ and then process it through a downstream MLP head to predict binding affinities $log\\tilde{K_d}$.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Overall architecture should be the following:\n",
        "\n",
        "#@markdown 1.) $h_p$ ~ $Encoder_{\\theta_p}(x_p)$ and $h_l$ ~ $Encoder_{\\theta_l}(x_l)$, where $x_p, x_l$ are the protein and ligand input data, $\\theta_p$, $\\theta_l$ are model parameters for protein sequences and ligand SMILES, and $h_p$, $h_l$ are hidden representations for protein and ligand sequences.\n",
        "\n",
        "#@markdown 2.) h = [$h_p$, $h_l$]; concatenate the representations into an individual tensor\n",
        "\n",
        "#@markdown 3.) y ~ $MLP_{\\theta_b}(h)$, where $\\theta_b$ is the model parameters for the downstream regression head and $y$ is the binding affinity output labels.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "class dual_encoder_MLP(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      protein_input_size: int,\n",
        "      protein_hidden_size: int,\n",
        "      protein_depth: int,\n",
        "      ligand_input_size: int,\n",
        "      ligand_hidden_size: int,\n",
        "      ligand_depth: int,\n",
        "      head_hidden_emb: int\n",
        "    ):\n",
        "\n",
        "    super(dual_encoder_MLP, self).__init__()\n",
        "\n",
        "    # define hyperparameters\n",
        "    self.protein_input_size = protein_input_size\n",
        "    self.protein_hidden_size = protein_hidden_size\n",
        "    self.protein_depth = protein_depth\n",
        "    self.ligand_input_size = ligand_input_size\n",
        "    self.ligand_hidden_size = ligand_hidden_size\n",
        "    self.ligand_depth = ligand_depth\n",
        "    self.head_hidden_emb = head_hidden_emb\n",
        "\n",
        "    # define architecture layers\n",
        "    self.protein_layers = self.protein_MLP() # h_p ~ Encoder(h_p | x_p)\n",
        "    self.ligand_layers = self.ligand_MLP() # h_l ~ Encoder(h_l | x_l)\n",
        "    self.head_layers = self.downstream_head_MLP() # y ~ MLP(y | h), where h = [h_p, h_l]\n",
        "\n",
        "\n",
        "  def protein_MLP(self,):\n",
        "      'note: similar architecture as simple_MLP without output layer'\n",
        "      protein_layers = nn.ModuleList()\n",
        "\n",
        "      # input layers\n",
        "      protein_layers.append(nn.Linear(\n",
        "          self.protein_input_size, self.protein_hidden_size\n",
        "      ))\n",
        "      protein_layers.append(nn.GELU())\n",
        "\n",
        "      # hidden layers\n",
        "      for _ in range(self.protein_depth - 1):\n",
        "        protein_layers.append(nn.Linear(\n",
        "                    self.protein_hidden_size, self.protein_hidden_size\n",
        "        ))\n",
        "        protein_layers.append(nn.GELU())\n",
        "\n",
        "      return protein_layers\n",
        "\n",
        "  def ligand_MLP(self,):\n",
        "      'note: similar architecture as simple_MLP without output layer'\n",
        "      ligand_layers = nn.ModuleList()\n",
        "\n",
        "      # input layers\n",
        "      ligand_layers.append(nn.Linear(\n",
        "          self.ligand_input_size, self.ligand_hidden_size\n",
        "      ))\n",
        "      ligand_layers.append(nn.GELU())\n",
        "\n",
        "      # hidden layers\n",
        "      for _ in range(self.ligand_depth - 1):\n",
        "        ligand_layers.append(nn.Linear(\n",
        "                    self.ligand_hidden_size, self.ligand_hidden_size\n",
        "        ))\n",
        "        ligand_layers.append(nn.GELU())\n",
        "\n",
        "      return ligand_layers\n",
        "\n",
        "  def downstream_head_MLP(self,):\n",
        "    'note: We will create the architecture for you, but you will need to figure out the right variables for the hyperparameters (e.g. input_dim)'\n",
        "\n",
        "    input_dim = self.protein_hidden_size + self.ligand_hidden_size # remember h = [h_p, h_l]\n",
        "    output_dim = 1 # regression task\n",
        "\n",
        "    ' MLP head for regression'\n",
        "    head_layers = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.head_hidden_emb),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(self.head_hidden_emb, 1)\n",
        "    )\n",
        "\n",
        "    return head_layers\n",
        "\n",
        "  # forward pass of neural network y ~ f(x)\n",
        "  def forward(self, x_p, x_l):\n",
        "    # Flatten the input tensor\n",
        "    x_p = x_p.view(x_p.size(0), -1) # protein data\n",
        "    x_l = x_l.view(x_l.size(0), -1) # ligand data\n",
        "\n",
        "    # infer hidden representations for proteins\n",
        "    h_p = x_p.clone()\n",
        "    for layer in self.protein_layers:\n",
        "      h_p = layer(h_p)\n",
        "\n",
        "    # infer hidden representations for ligands\n",
        "    h_l = x_l.clone()\n",
        "    for layer in self.ligand_layers:\n",
        "      h_l = layer(h_l)\n",
        "\n",
        "    # concatenate representations\n",
        "    h = torch.cat((h_p, h_l), dim=-1)\n",
        "\n",
        "    # predict regression values\n",
        "    y_pred = self.head_layers(h)\n",
        "\n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVd-TXAFwZZv"
      },
      "outputs": [],
      "source": [
        "#@title Define the protein sequence-based model for affinity prediction\n",
        "\n",
        "' Protein encoder hyperparameters '\n",
        "protein_seq_len = train_x_proteins.shape[1] # sequence length\n",
        "protein_char_tokens = train_x_proteins.shape[-1] # number of protein sequence tokens\n",
        "protein_input_size=protein_seq_len * protein_char_tokens # input dimension to MLP\n",
        "protein_hidden_size=500 # hyperparameter; control expressive the neural network is...\n",
        "protein_depth=2 # hyperparameter; control how deep the neural network is...\n",
        "\n",
        "' Ligand encoder hyperparameters '\n",
        "ligand_seq_len = train_x_ligands.shape[1] # sequence length\n",
        "ligand_char_tokens = train_x_ligands.shape[-1] # number of ligand sequence tokens\n",
        "ligand_input_size=ligand_seq_len * ligand_char_tokens # input dimension to MLP\n",
        "ligand_hidden_size=500 # hyperparameter; control expressive the neural network is...\n",
        "ligand_depth=2 # hyperparameter; control how deep the neural network is...\n",
        "\n",
        "' head hyperparameters'\n",
        "head_hidden_emb=500 # hyperparameter; control expressive the neural network is..\n",
        "\n",
        "\n",
        "# create model\n",
        "protein_ligand_model = dual_encoder_MLP(\n",
        "    protein_input_size=protein_input_size,\n",
        "    protein_hidden_size=protein_hidden_size,\n",
        "    protein_depth=protein_depth,\n",
        "    ligand_input_size=ligand_input_size,\n",
        "    ligand_hidden_size=ligand_hidden_size,\n",
        "    ligand_depth=ligand_depth,\n",
        "    head_hidden_emb=head_hidden_emb\n",
        ")\n",
        "protein_ligand_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7Svx9DvBz2B7"
      },
      "outputs": [],
      "source": [
        "#@title Function for compute metrics on test set for dual encoder üõ†Ô∏è\n",
        "@torch.no_grad()\n",
        "def test_performance_metrics_on_dual(model: nn.Module, dataloader: DataLoader):\n",
        "\n",
        "  model.eval()\n",
        "  # Define a loss function\n",
        "  criterion = nn.MSELoss()  # For simple regression prediction, we can use mean_squared_error loss.\n",
        "\n",
        "  y_true_batch, y_pred_batch = [], []\n",
        "\n",
        "  for i, (x_p, x_l, y_true) in enumerate(dataloader):\n",
        "\n",
        "    # Move inputs and labels to the device\n",
        "    x_p, x_l, y_true = x_p.to(device), x_l.to(device), y_true.to(device)\n",
        "\n",
        "    # Forward propagation\n",
        "    y_pred = model(x_p=x_p, x_l=x_l).squeeze(1)\n",
        "\n",
        "    y_true_batch.append(y_true)\n",
        "    y_pred_batch.append(y_pred)\n",
        "\n",
        "  # concatenate batches\n",
        "  y_true_batch = torch.cat(y_true_batch, dim=0)\n",
        "  y_pred_batch = torch.cat(y_pred_batch, dim=0)\n",
        "\n",
        "  # Loss computation\n",
        "  loss = criterion(y_pred_batch, y_true_batch)\n",
        "\n",
        "  # get metric performance\n",
        "  pearson_corr, spearman_rho = performance_metrics(\n",
        "                y_true=y_true_batch,\n",
        "                y_pred=y_pred_batch\n",
        "  )\n",
        "  print(f\"Test metrics | Loss: {loss.item():.3f}, Pearson: {pearson_corr.item():.3f}, Spearman rho: {spearman_rho.item():.3f}\")\n",
        "\n",
        "  return (\n",
        "      loss.item(),\n",
        "      pearson_corr.item(),\n",
        "      spearman_rho.item()\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClqFwokyy9oM"
      },
      "outputs": [],
      "source": [
        "#@title Train the Protein-ligand sequence-based model üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
        "\n",
        "#@title Train the ligand model\n",
        "\n",
        "# Define a loss function\n",
        "criterion = nn.MSELoss()  # For simple regression prediction, we can use mean_squared_error loss.\n",
        "\n",
        "# define an optimizer\n",
        "optimizer = optim.Adam(protein_ligand_model.parameters(), lr=0.0001)  # lr corresponds to learning_rate (keep fixed for now)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = protein_ligand_model.to(device)\n",
        "\n",
        "\n",
        "#@markdown Since ligand-based encoder takes quite some time, we will load the weights of a pretrained model instead of train.\n",
        "\n",
        "\n",
        "train_model_option = 'load_weights' #@param ['train', 'load_weights']{type:\"string\"}\n",
        "\n",
        "# number of training cycles over the dataset (i.e. epochs)\n",
        "num_epochs = 0 #@param [0, 1, 10, 15, 30] {type:\"raw\"}\n",
        "\n",
        "\n",
        "\n",
        "# track losses and metrics\n",
        "train_protein_ligand_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "test_protein_ligand_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "iteration = 0\n",
        "# Start the training loop\n",
        "for epoch in range(num_epochs):  # num_epochs is the number of epochs to train for\n",
        "    for i, (x_protein, x_ligand, y_true) in enumerate(train_protein_ligand_dataloader ):\n",
        "        # Move inputs and labels to the device\n",
        "        x_protein, x_ligand, y_true = x_protein.to(device), x_ligand.to(device), y_true.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        y_pred = protein_ligand_model(x_p=x_protein, x_l=x_ligand).squeeze(1)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "        pearson_corr, spearman_rho = performance_metrics(\n",
        "                y_true=y_true,\n",
        "                y_pred=y_pred\n",
        "        )\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_protein_ligand_dataloader)}, Loss: {loss.item():.3f}, Pearson: {pearson_corr.item():.3f}, Spearman rho: {spearman_rho.item():.3f}\")\n",
        "\n",
        "\n",
        "        # log training performance\n",
        "        train_protein_ligand_history_dict['step'].append(iteration)\n",
        "        iteration+=1\n",
        "        train_protein_ligand_history_dict['loss'].append(loss.item())\n",
        "        train_protein_ligand_history_dict['pearson_corr'].append(pearson_corr.item())\n",
        "        train_protein_ligand_history_dict['spearman_rho'].append(spearman_rho.item())\n",
        "\n",
        "    # compute performance metrics on the test set\n",
        "    test_loss, test_pearson_corr, test_spearman_rho = test_performance_metrics_on_dual(model=protein_ligand_model, dataloader=test_protein_ligand_dataloader)\n",
        "    test_protein_ligand_history_dict['step'].append(iteration)\n",
        "    test_protein_ligand_history_dict['loss'].append(test_loss)\n",
        "    test_protein_ligand_history_dict['pearson_corr'].append(test_pearson_corr)\n",
        "    test_protein_ligand_history_dict['spearman_rho'].append(test_spearman_rho)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gS9ITqa2opy"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training history üîé\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10,3))\n",
        "\n",
        "axes[0].plot(train_protein_ligand_history_dict['step'], train_protein_ligand_history_dict['loss'])\n",
        "axes[0].plot(test_protein_ligand_history_dict['step'], test_protein_ligand_history_dict['loss'], '.-', markersize=10)\n",
        "axes[0].set_ylabel('loss')\n",
        "axes[0].set_xlabel('steps')\n",
        "\n",
        "axes[1].plot(train_protein_ligand_history_dict['step'], train_protein_ligand_history_dict['pearson_corr'])\n",
        "axes[1].plot(test_protein_ligand_history_dict['step'], test_protein_ligand_history_dict['pearson_corr'], '.-', markersize=10)\n",
        "axes[1].set_ylabel('Pearson corr.')\n",
        "axes[1].set_xlabel('steps')\n",
        "\n",
        "axes[2].plot(train_protein_ligand_history_dict['step'], train_protein_ligand_history_dict['spearman_rho'])\n",
        "axes[2].plot(test_protein_ligand_history_dict['step'], test_protein_ligand_history_dict['spearman_rho'], '.-', markersize=10)\n",
        "axes[2].set_ylabel('Spearman rho')\n",
        "axes[2].set_xlabel('steps')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llMFCNIoqmxo"
      },
      "outputs": [],
      "source": [
        "#@title #### Problem: can you use the loaded pretrain model to predict $log( norm(K_d))$ and compare against ground truth? üö©\n",
        "\n",
        "#@markdown Note: you can compute statistics like pearson correlation or spearmann $\\rho$ correlation too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "khUuVu1OT03a"
      },
      "outputs": [],
      "source": [
        "#@title Compare performance between the 3 approaches on the test set.\n",
        "\n",
        "pretrained_p_seq_test_dict = torch.load('/content/UChicago_AIplusScience_code_workshops/pretrained_weights/test_Protein_sequence_history.pt')\n",
        "pretrained_l_seq_test_dict = torch.load('/content/UChicago_AIplusScience_code_workshops/pretrained_weights/test_Ligand_sequence_history.pt')\n",
        "pretrained_pl_seq_test_dict = torch.load('/content/UChicago_AIplusScience_code_workshops/pretrained_weights/test_Protein_Ligand_sequence_history.pt')\n",
        "\n",
        "\n",
        "plt.bar(\n",
        "    ['ligand encoder', 'protein encoder', 'protein-ligand \\n dual encoder'],\n",
        "    [pretrained_p_seq_test_dict['spearman_rho'][-1], pretrained_l_seq_test_dict['spearman_rho'][-1], pretrained_pl_seq_test_dict['spearman_rho'][-1]]\n",
        ")\n",
        "plt.ylabel('Spearmann rho on test set.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BxT-tQMq7Bo"
      },
      "source": [
        "# Problem: why did the dual encoder that takes both protein and ligand sequences performs better than the single encoder models? üö©\n",
        "\n",
        "\n",
        "## $<$Insert anwser here$>$\n",
        "\n",
        "Note: think about what the actual problem..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPqQh1_2UIyo"
      },
      "source": [
        "## Sequence and structure-based encoder model\n",
        "\n",
        "The next modeling arhcitecture will incorporate both the sequence-based information for both the protein and ligand sequence, while also introducing 3D structure data corresponding to the atom coordinates for the protein structure and ligand molecule. The hope is that the additional 3D data from the protein and ligand alongside the sequence information will improve binding affinity prediction on the test set. To process the 3D into representations, we will need to incorporate architectures that fall into the category of `Geometric deep learning`. This field has emerge as a exciting monolith in deep learning the last few yeasrs, with many successes for proteins and molecules (from protein structure prediction to molecule design). Here are some useful resources on geometric deep learing:\n",
        "\n",
        "* blog: https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d\n",
        "* book: https://arxiv.org/abs/2104.13478\n",
        "* video: https://www.youtube.com/watch?v=w6Pw4MOzMuo\n",
        "    \n",
        "   \n",
        "Architectures for geometric deep learning can range from simple convolution neural networks to sophisicated graph neural networks. For our implementation and a simple introduce to working on 3D data, we will implement PointNet neural networks, which are models that effectively learn on pointclouds.\n",
        "\n",
        "Here is an illustration that shows a 3D bed rendering that is converted into a pointcloud data type, which is then processed by a PointNet model and predicts the class label `bed`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d9T9go7BUGpj"
      },
      "outputs": [],
      "source": [
        "#@title Point clouds allow to model 3D data.\n",
        "from IPython.display import Image, display\n",
        "\n",
        "image_path = \"/content/UChicago_AIplusScience_code_workshops/figures/pointnet_video_illustration.gif\"\n",
        "display(Image(filename=image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIlXvCRBUZ45"
      },
      "source": [
        "The exact details of the architecture of PointNet are not necessary for our current implementation; however, it is important to know that PointNet is permutation invariant which is well suited for working on cartesian coordinates. The main reasoning why PointNet achieves permutation invariance is thanks to the aggregated layer -- either a max pooling or average pooling operation. (run cell below). More details on PointNet can be found at the following links:\n",
        "\n",
        "* Original paper: https://arxiv.org/abs/1612.00593\n",
        "* Blog: http://stanford.edu/~rqi/pointnet/\n",
        "* Notebook example from torch.geometric (Graph classification): https://colab.research.google.com/drive/1D45E5bUK3gQ40YpZo65ozs7hg5l-eo_U?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FWCS4tAoUW1K"
      },
      "outputs": [],
      "source": [
        "#@title Global pooling layer for achieving permutation invariance.\n",
        "image_path = \"/content/UChicago_AIplusScience_code_workshops/figures/symmetric_function.PNG\"\n",
        "display(Image(filename=image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tksF8z9VMQm"
      },
      "source": [
        "`Goal:` Fe will need to create a dataset iterator that processes the protein sequences, ligand SMILES, and now both the protein and ligand atom coordinates as 3D data (i.e. point cloud).\n",
        "\n",
        "`Note:` For this task, we will provide the architecture model and dataset iterator, but we will require for you to setup the forward pass hook for the PyTorch model.\n",
        "\n",
        "`Second Note:` Working with 4 encoders and training a model on 3D data can be quite expensive, and thus we will provide a complete pretrained model. We will require you to create a function that infers and predicts binding affinities on test set. Then using the predicted measurements, we will ask you to compute performance metrics against the ground truth (e.g. $R^2$, etc).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f5DdBAwnVJMC"
      },
      "outputs": [],
      "source": [
        "#@title Helpful functions to remove clutter from warning messages. üõ†Ô∏è\n",
        "\n",
        "import warnings\n",
        "from Bio import BiopythonWarning\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=BiopythonWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLiGc-e6WAWB"
      },
      "outputs": [],
      "source": [
        "#@title Create a dataloader that processes sequence and structure data. üõ†Ô∏è\n",
        "\n",
        "#@markdown `note:` We will provide `structure_sequence_dataset` dataset iterator for you since the object is quite sophisticated. However, please analyze the `__init__()` hook to understand what data variables are needed to process the dataset, and the `__getitem__()` hook to understand which data enters and exits object.\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "from Bio.PDB import PDBParser\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class structure_sequence_dataset(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      protein_seq_data: list,\n",
        "      protein_max_seq_len: int,\n",
        "      protein_structure_paths: list,\n",
        "      ligand_seq_data: list,\n",
        "      ligand_max_seq_len: int,\n",
        "      ligand_structure_paths: list,\n",
        "      output_labels: list,\n",
        "      ):\n",
        "\n",
        "    # binding affinity measurements\n",
        "    self.output_labels = output_labels\n",
        "\n",
        "    ' preprocess protein sequence data'\n",
        "    self.protein_data = protein_seq_data # input protein sequences\n",
        "    self.protein_max_seq_len = protein_max_seq_len # max sequence length\n",
        "    # Define the tokens to cover all the possible amino acids including special ones\n",
        "    self.protein_tokens = [\n",
        "        'A', 'C', 'D', 'E', 'F', 'G', 'H',\n",
        "        'I', 'K', 'L', 'M', 'N', 'P', 'Q',\n",
        "        'R', 'S', 'T', 'V', 'W', 'Y', 'U',\n",
        "        'O', '-'\n",
        "    ]\n",
        "    # setup the amino acid tokenizer\n",
        "    self.protein_tokenizer = {token:ii for ii, token in enumerate(self.protein_tokens)}\n",
        "\n",
        "    ' preprocess protein structure data'\n",
        "    # process PDB structure\n",
        "    self.parser = PDBParser() # comes from BioPython package (loads PDB structures)\n",
        "    self.protein_paths = protein_structure_paths # structure paths\n",
        "\n",
        "    ' preprocess ligand sequence data'\n",
        "    self.ligand_data = ligand_seq_data # input ligand SMILES sequences\n",
        "    self.ligand_max_seq_len = ligand_max_seq_len # max sequence length\n",
        "    # ligand SMILEs tokenization\n",
        "    self.ligand_tokenizer = dc.feat.OneHotFeaturizer() # ligand SMILEs tokenization\n",
        "\n",
        "    ' preprocess ligand structure data'\n",
        "    self.ligand_paths = ligand_structure_paths # structure paths\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    assert len(self.protein_data) == len(self.ligand_data), \"Length of protein_data and ligand_data should be equal\"\n",
        "    return len(self.protein_data)\n",
        "\n",
        "  def preprocess_protein_sequence(self, protein_sample):\n",
        "      # convert amino acid characters into numerical values\n",
        "      num_protein_sample = [self.protein_tokenizer[aa_char] for aa_char in protein_sample]\n",
        "      # one hot encoding\n",
        "      one_hot = F.one_hot(torch.tensor(num_protein_sample), num_classes=len(self.protein_tokens))\n",
        "      # compute the padding length\n",
        "      pad_len = self.protein_max_seq_len - one_hot.size(0)\n",
        "      # create a tensor of zeros of shape (pad_len, len(self.tokens))\n",
        "      pad_tensor = torch.zeros(pad_len, len(self.protein_tokens)).long()\n",
        "      # set the padding tensor with the 'PAD' index\n",
        "      pad_tensor[:, self.protein_tokenizer['-']] = 1\n",
        "      # concatenate features and pad_tensor along dimension 0\n",
        "      x_padded = torch.cat((one_hot, pad_tensor), dim=0)\n",
        "      return x_padded\n",
        "\n",
        "  def preprocess_smiles_sequence(self, smiles_sample):\n",
        "\n",
        "    # convert string characters to one hot encoded tensors\n",
        "    features =  torch.Tensor(self.ligand_tokenizer.featurize(smiles_sample)[:,0,:])\n",
        "    # compute the padding length\n",
        "    pad_len = self.ligand_max_seq_len - features.size(0)\n",
        "    # create a tensor of zeros of shape (pad_len, 35)\n",
        "    pad_tensor = torch.zeros(pad_len, 35)\n",
        "    # set the last dimension of each vector in pad tensor to 1 (i.e. padded class)\n",
        "    pad_tensor[:, -1] = 1\n",
        "    # concatenate features and pad_tensor along dimension 0\n",
        "    x_padded = torch.cat((features, pad_tensor), dim=0)\n",
        "    return x_padded\n",
        "\n",
        "  def pad_point_clouds(self, point_clouds, max_points):\n",
        "        \"\"\"\n",
        "        we need to paddded 0s to a fixed size tensor for processing pytorch tools and models.\n",
        "        \"\"\"\n",
        "\n",
        "        # pad the point clouds with a default value\n",
        "        padded_point_clouds = []\n",
        "        default_values = torch.tensor([0.0, 0.0, 0.0]) # default coordinates for the pads\n",
        "\n",
        "        #for point_cloud in point_clouds:\n",
        "        num_points = point_clouds.size(0)\n",
        "        padding_size = max_points - num_points\n",
        "\n",
        "        # pad point cloud\n",
        "        padded_point_cloud = torch.cat([point_clouds, default_values.repeat(padding_size, 1)])\n",
        "\n",
        "        # padded_point_clouds is now a list of point cloud tensors with the same number of points (max_points)\n",
        "        return padded_point_cloud\n",
        "\n",
        "  def preprocess_protein_structure(self, protein_path):\n",
        "\n",
        "        structure = self.parser.get_structure(\"protein\", protein_path)\n",
        "        point_cloud = []\n",
        "\n",
        "        # extract the atom coordinates\n",
        "        for model in structure:\n",
        "            for chain in model:\n",
        "                for residue in chain:\n",
        "                    for atom in residue:\n",
        "                        if atom.get_name() == \"CA\": # consider only C-alpha atoms for backbone\n",
        "                            coord = atom.get_coord()\n",
        "                            point_cloud.append(coord)\n",
        "\n",
        "        # assemble point cloud from atom coordinates as torch tensor\n",
        "        point_cloud = torch.from_numpy(np.array(point_cloud)).float()\n",
        "\n",
        "        # Center the point cloud to the center of mass\n",
        "        center_of_mass = torch.mean(point_cloud, axis=0)\n",
        "        centered_point_cloud = point_cloud - center_of_mass\n",
        "        # pad point clouds to have save size tensors\n",
        "        padded_point_cloud = self.pad_point_clouds(\n",
        "            point_clouds=centered_point_cloud,\n",
        "            max_points=5000\n",
        "        )\n",
        "        # note: torch.geometric Data object has several attributes, making it useful for us. We will only use 'pos' for point clouds.\n",
        "        protein_structure_data = Data(pos=padded_point_cloud)\n",
        "\n",
        "        # to access point cloud coordinates: protein_structure_data.pos\n",
        "        return protein_structure_data\n",
        "\n",
        "\n",
        "\n",
        "  def preprocess_ligand_structure(self, ligand_path):\n",
        "\n",
        "        # load the molecule from the file\n",
        "        mol = Chem.MolFromMol2File(ligand_path)\n",
        "\n",
        "\n",
        "        if mol is None:\n",
        "            # handle the case where molecule loading fails\n",
        "            atom_coordinates = torch.zeros((0, 3), dtype=torch.float)\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                # extract the ligand atoms coordinates\n",
        "                atom_coordinates = torch.tensor(\n",
        "                        mol.GetConformer().GetPositions()\n",
        "                )\n",
        "            except AttributeError:\n",
        "                # Handle the case where conformer or positions are missing\n",
        "                atom_coordinates = torch.zeros((0,3), dtype=torch.float)\n",
        "\n",
        "         # Center the point cloud to the center of mass\n",
        "        center_of_mass = torch.mean(atom_coordinates, axis=0)\n",
        "        centered_point_cloud = atom_coordinates - center_of_mass\n",
        "\n",
        "        # pad atom coordinates\n",
        "        pad_atom_coordinates = self.pad_point_clouds(\n",
        "            point_clouds=centered_point_cloud,\n",
        "            max_points=500\n",
        "        )\n",
        "        # note: torch.geometric Data object has a several attributes, making it useful for us. We will only use 'pos' for point clouds.\n",
        "        ligand_structure_data = Data(pos=pad_atom_coordinates)\n",
        "\n",
        "        return ligand_structure_data\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    ' preprocess protein sequence data'\n",
        "    # prepare input data\n",
        "    protein_sample = self.protein_data[idx]\n",
        "    x_protein_seq = self.preprocess_protein_sequence(protein_sample=protein_sample)\n",
        "\n",
        "    ' preprocess protein structure data'\n",
        "    # prepare structure input data\n",
        "    protein_paths = self.protein_paths[idx]\n",
        "    x_protein_structures = self.preprocess_protein_structure(protein_path=protein_paths)\n",
        "    x_protein_point_cloud = x_protein_structures.pos # extract point cloud\n",
        "\n",
        "    ' preprocess ligand data'\n",
        "    # prepare input data\n",
        "    ligands = self.ligand_data[idx]\n",
        "    x_ligand_seq = self.preprocess_smiles_sequence(smiles_sample=ligands)\n",
        "\n",
        "    'preproces ligand structure data'\n",
        "    ligand_paths = self.ligand_paths[idx]\n",
        "    x_ligand_structures = self.preprocess_ligand_structure(ligand_path=ligand_paths)\n",
        "    x_ligand_point_cloud = x_ligand_structures.pos # extract point cloud\n",
        "\n",
        "    ' preprocess output label data'\n",
        "    # prepare output/prediction data\n",
        "    y_true = torch.tensor(self.output_labels[idx])\n",
        "\n",
        "    return (\n",
        "        x_protein_seq.float(),\n",
        "        x_protein_point_cloud.float(),\n",
        "        x_ligand_seq.float(),\n",
        "        x_ligand_point_cloud.float(),\n",
        "        y_true\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "95gWKxRIXqnQ"
      },
      "outputs": [],
      "source": [
        "#@title Reminder that we will need to use dataframe columns to define `structure_sequence_dataset` object\n",
        "\n",
        "# here are the columns for the training set\n",
        "display(train_Kd_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN3OXPz4W4fp"
      },
      "outputs": [],
      "source": [
        "#@title Problem: define the `structure_sequence_dataset` using the train_Kd_df and test_Kd_df dataframes  üö©\n",
        "\n",
        "#@markdown `Note:` To understand which data variables to define `structure_sequence_dataset` iterator, please analyze `__init__()` inputs\n",
        "\n",
        "#@markdown `Second Note:` Remember, we need data corresponding the protein sequence, ligand SMILES string, protein structure 3D data, and ligand 3D data.\n",
        "\n",
        "#@markdown `Third Note:` To load structure data, we will need to leverage data paths and load structure per batch to avoid Out Of Memory issues (OOM). I.e., use columns `protein_pdb_paths` and `ligand_mol_paths`.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "' set up sequence+structure training dataset with Dataset and Dataloader using pytorch'\n",
        "train_seq_struct_dataset = structure_sequence_dataset(\n",
        "    protein_seq_data=train_Kd_df['protein_sequence'].tolist(),\n",
        "    protein_max_seq_len=max(len(s) for s in train_Kd_df['protein_sequence'].tolist()),\n",
        "    protein_structure_paths=train_Kd_df['protein_pdb_paths'],\n",
        "    ligand_seq_data=train_Kd_df['ligand_smiles'].tolist(),\n",
        "    ligand_max_seq_len=max(len(s) for s in train_Kd_df['ligand_smiles'].tolist()),\n",
        "    ligand_structure_paths=train_Kd_df['ligand_mol_paths'],\n",
        "    output_labels=train_Kd_df['log_norm_Kd'].tolist(),\n",
        ")\n",
        "train_seq_struct_dataloader = DataLoader(\n",
        "        train_seq_struct_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,# shuffle is only 'True' for training sets.\n",
        "        num_workers=0\n",
        ")\n",
        "' set up sequence+structure testing dataset with Dataset and Dataloader using pytorch'\n",
        "\n",
        "test_seq_struct_dataset = structure_sequence_dataset(\n",
        "    protein_seq_data=test_Kd_df['protein_sequence'].tolist(),\n",
        "    protein_max_seq_len=max(len(s) for s in train_Kd_df['protein_sequence'].tolist()),\n",
        "    protein_structure_paths=test_Kd_df['protein_pdb_paths'],\n",
        "    ligand_seq_data=test_Kd_df['ligand_smiles'].tolist(),\n",
        "    ligand_max_seq_len=max(len(s) for s in train_Kd_df['ligand_smiles'].tolist()),\n",
        "    ligand_structure_paths=train_Kd_df['ligand_mol_paths'],\n",
        "    output_labels=test_Kd_df['log_norm_Kd'].tolist(),\n",
        ")\n",
        "test_seq_struct_dataloader = DataLoader(\n",
        "        test_seq_struct_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False, # shuffle is only 'True' for training sets.\n",
        "        num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QOBMSN9ZLcG"
      },
      "outputs": [],
      "source": [
        "#@title Sanity check: example of running a single forward pass through the dataloaders. üîé\n",
        "\n",
        "\n",
        "' here we will run a single pass through the data loader as an example '\n",
        "train_x_protein_seq, train_x_protein_cloud, train_x_ligand_seq, train_x_ligand_cloud, train_y_true = next(iter(train_seq_struct_dataloader))\n",
        "\n",
        "print('Training set:')\n",
        "print('='*50)\n",
        "print('protein sequence shape (batch_size, sequence_length, class_labels) -> ', list(train_x_protein_seq.shape))\n",
        "print('protein structure shape (batch_size, C-alpha_atoms, 3D_coordinates) -> ', list(train_x_protein_cloud.shape))\n",
        "print('ligand sequence shape (batch_size, sequence_length, class_labels) -> ', list(train_x_ligand_seq.shape))\n",
        "print('ligand structure shape (batch_size, atoms, 3D_coordinates) -> ', list(train_x_ligand_cloud.shape))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "' here we will run a single pass through the data loader as an example '\n",
        "test_x_protein_seq, test_x_protein_cloud, test_x_ligand_seq, test_x_ligand_cloud, test_y_true = next(iter(test_seq_struct_dataloader))\n",
        "\n",
        "print('\\nTesting set:')\n",
        "print('='*50)\n",
        "print('protein sequence shape (batch_size, sequence_length, class_labels) -> ', list(test_x_protein_seq.shape))\n",
        "print('protein structure shape (batch_size, C-alpha_atoms, 3D_coordinates) -> ', list(test_x_protein_cloud.shape))\n",
        "print('ligand sequence shape (batch_size, sequence_length, class_labels) -> ', list(test_x_ligand_seq.shape))\n",
        "print('ligand structure shape (batch_size, atoms, 3D_coordinates) -> ', list(test_x_ligand_cloud.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S8BsIcp_Zn64"
      },
      "outputs": [],
      "source": [
        "#@title Functions for analyzing and visualizing point clouds. üõ†Ô∏èüîé\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from ipywidgets import interactive\n",
        "\n",
        "def plot_point_cloud(angle):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    # Generate random point cloud data\n",
        "\n",
        "    # Extract x, y, and z coordinates from the point cloud\n",
        "    x = temp_point_cloud[:, 0]\n",
        "    y = temp_point_cloud[:, 1]\n",
        "    z = temp_point_cloud[:, 2]\n",
        "\n",
        "    # Plot the point cloud\n",
        "    ax.scatter(x, y, z, c=\"b\", marker=\"o\")\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"Y\")\n",
        "    ax.set_zlabel(\"Z\")\n",
        "    ax.set_title(\"Point Cloud\")\n",
        "\n",
        "    # Rotate the plot\n",
        "    ax.view_init(elev=angle, azim=angle)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HZqdRiJ7Z3FG"
      },
      "outputs": [],
      "source": [
        "#@title Visualize proteins as point clouds. üîé\n",
        "\n",
        "\n",
        "sample_index=4#@param{type:\"integer\"}\n",
        "train_set=True#@param {type:\"boolean\"} # generate from the training\n",
        "\n",
        "if train_set:\n",
        "  temp_point_cloud = train_x_protein_cloud[sample_index]\n",
        "  visualize_protein(pdb_path=train_Kd_df['protein_pdb_paths'].loc[sample_index])\n",
        "  print(f\"Binding affinity: {train_Kd_df['exp_measurement'].loc[sample_index]}\")\n",
        "else:\n",
        "  temp_point_cloud = test_x_protein_cloud[sample_index]\n",
        "  visualize_protein(pdb_path=test_Kd_df['protein_pdb_paths'].loc[sample_index])\n",
        "  print(f\"Binding affinity: {test_Kd_df['exp_measurement'].loc[sample_index]}\")\n",
        "\n",
        "\n",
        "\n",
        "# Create an interactive slider\n",
        "angle_slider = interactive(\n",
        "    plot_point_cloud,\n",
        "    angle=(0, 360, 5)\n",
        ")\n",
        "display(angle_slider)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IJLeBymWaDOV"
      },
      "outputs": [],
      "source": [
        "#@title Visualize ligands as point clouds. üîé\n",
        "\n",
        "\n",
        "\n",
        "sample_index=12#@param{type:\"integer\"}\n",
        "train_set=True#@param {type:\"boolean\"} # generate from the training\n",
        "\n",
        "if train_set:\n",
        "  temp_point_cloud = train_x_ligand_cloud[sample_index]\n",
        "  visualize_ligand(ligand_sdf_path=train_Kd_df['ligand_sdf_paths'].loc[sample_index])\n",
        "  print(f\"Binding affinity: {train_Kd_df['exp_measurement'].loc[sample_index]}\")\n",
        "else:\n",
        "  temp_point_cloud = test_x_ligand_cloud[sample_index]\n",
        "  visualize_ligand(ligand_sdf_path=test_Kd_df['ligand_sdf_paths'].loc[sample_index])\n",
        "  print(f\"Binding affinity: {test_Kd_df['exp_measurement'].loc[sample_index]}\")\n",
        "\n",
        "# Create an interactive slider\n",
        "angle_slider = interactive(\n",
        "    plot_point_cloud,\n",
        "    angle=(0, 360, 5)\n",
        ")\n",
        "display(angle_slider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMgf0kAcbMnB"
      },
      "source": [
        "\n",
        "# Problem: Create a dual encoder MLP that each processes protein and ligand data.  üö©\n",
        "\n",
        "Here, we will create and define the model architecture, but you will need to define the forward pass. In other words, you will need to setup which samples enter the model and which samples exit the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ybfrVOJTbL3c"
      },
      "outputs": [],
      "source": [
        "#@title Functions for PointNet. üõ†Ô∏è\n",
        "\n",
        "#@markdown `note:` There is no need to understand all of the details of PointNet for brevity. It is important to understand that PointNet allows to process 3D data (e.g. PointCloud), but there is many alternative approaches like Graph Neural Networks, 3D Convolutional Neural Networks (voxelization).\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "import scipy.spatial\n",
        "\n",
        "def knn(x, y, k, batch_x=None, batch_y=None):\n",
        "    r\"\"\"Finds for each element in :obj:`y` the :obj:`k` nearest points in\n",
        "    :obj:`x`.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Node feature matrix\n",
        "            :math:`\\mathbf{X} \\in \\mathbb{R}^{N \\times F}`.\n",
        "        y (Tensor): Node feature matrix\n",
        "            :math:`\\mathbf{X} \\in \\mathbb{R}^{M \\times F}`.\n",
        "        k (int): The number of neighbors.\n",
        "        batch_x (LongTensor, optional): Batch vector\n",
        "            :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns each\n",
        "            node to a specific example. (default: :obj:`None`)\n",
        "        batch_y (LongTensor, optional): Batch vector\n",
        "            :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^M`, which assigns each\n",
        "            node to a specific example. (default: :obj:`None`)\n",
        "\n",
        "    :rtype: :class:`LongTensor`\n",
        "\n",
        "    .. testsetup::\n",
        "\n",
        "        import torch\n",
        "        from torch_cluster import knn\n",
        "\n",
        "    .. testcode::\n",
        "\n",
        "        >>> x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
        "        >>> batch_x = torch.tensor([0, 0, 0, 0])\n",
        "        >>> y = torch.Tensor([[-1, 0], [1, 0]])\n",
        "        >>> batch_x = torch.tensor([0, 0])\n",
        "        >>> assign_index = knn(x, y, 2, batch_x, batch_y)\n",
        "    \"\"\"\n",
        "\n",
        "    if batch_x is None:\n",
        "        batch_x = x.new_zeros(x.size(0), dtype=torch.long)\n",
        "\n",
        "    if batch_y is None:\n",
        "        batch_y = y.new_zeros(y.size(0), dtype=torch.long)\n",
        "\n",
        "    x = x.view(-1, 1) if x.dim() == 1 else x\n",
        "    y = y.view(-1, 1) if y.dim() == 1 else y\n",
        "\n",
        "    assert x.dim() == 2 and batch_x.dim() == 1\n",
        "    assert y.dim() == 2 and batch_y.dim() == 1\n",
        "    assert x.size(1) == y.size(1)\n",
        "    assert x.size(0) == batch_x.size(0)\n",
        "    assert y.size(0) == batch_y.size(0)\n",
        "\n",
        "   # if x.is_cuda:\n",
        "   #     return torch_cluster.knn_cuda.knn(x, y, k, batch_x, batch_y)\n",
        "\n",
        "    # Rescale x and y.\n",
        "    min_xy = min(x.min().item(), y.min().item())\n",
        "    x, y = x - min_xy, y - min_xy\n",
        "\n",
        "    max_xy = max(x.max().item(), y.max().item())\n",
        "    x, y, = x / max_xy, y / max_xy\n",
        "\n",
        "    # Concat batch/features to ensure no cross-links between examples exist.\n",
        "    x = torch.cat([x, 2 * x.size(1) * batch_x.view(-1, 1).to(x.dtype)], dim=-1)\n",
        "    y = torch.cat([y, 2 * y.size(1) * batch_y.view(-1, 1).to(y.dtype)], dim=-1)\n",
        "\n",
        "    tree = scipy.spatial.cKDTree(x.detach().numpy())\n",
        "    dist, col = tree.query(\n",
        "        y.detach().cpu(), k=k, distance_upper_bound=x.size(1))\n",
        "    dist = torch.from_numpy(dist).to(x.dtype)\n",
        "    col = torch.from_numpy(col).to(torch.long)\n",
        "    row = torch.arange(col.size(0), dtype=torch.long).view(-1, 1).repeat(1, k)\n",
        "\n",
        "    mask = ~torch.isinf(dist).view(-1) # using the ~ operator\n",
        "#    mask = 1 - torch.isinf(dist).view(-1)\n",
        "    row, col = row.view(-1)[mask], col.view(-1)[mask]\n",
        "\n",
        "    return torch.stack([row, col], dim=0)\n",
        "\n",
        "\n",
        "\n",
        "def knn_graph(x, k, batch=None, loop=False, flow='source_to_target'):\n",
        "    r\"\"\"Computes graph edges to the nearest :obj:`k` points.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Node feature matrix\n",
        "            :math:`\\mathbf{X} \\in \\mathbb{R}^{N \\times F}`.\n",
        "        k (int): The number of neighbors.\n",
        "        batch (LongTensor, optional): Batch vector\n",
        "            :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns each\n",
        "            node to a specific example. (default: :obj:`None`)\n",
        "        loop (bool, optional): If :obj:`True`, the graph will contain\n",
        "            self-loops. (default: :obj:`False`)\n",
        "        flow (string, optional): The flow direction when using in combination\n",
        "            with message passing (:obj:`\"source_to_target\"` or\n",
        "            :obj:`\"target_to_source\"`). (default: :obj:`\"source_to_target\"`)\n",
        "\n",
        "    :rtype: :class:`LongTensor`\n",
        "\n",
        "    .. testsetup::\n",
        "\n",
        "        import torch\n",
        "        from torch_cluster import knn_graph\n",
        "\n",
        "    .. testcode::\n",
        "\n",
        "        >>> x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
        "        >>> batch = torch.tensor([0, 0, 0, 0])\n",
        "        >>> edge_index = knn_graph(x, k=2, batch=batch, loop=False)\n",
        "    \"\"\"\n",
        "\n",
        "    assert flow in ['source_to_target', 'target_to_source']\n",
        "    row, col = knn(x, x, k if loop else k + 1, batch, batch)\n",
        "    row, col = (col, row) if flow == 'source_to_target' else (row, col)\n",
        "    if not loop:\n",
        "        mask = row != col\n",
        "        row, col = row[mask], col[mask]\n",
        "    return torch.stack([row, col], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bqYklh47cFe7"
      },
      "outputs": [],
      "source": [
        "#@title Layers for PointNet. üõ†Ô∏è\n",
        "\n",
        "#@markdown `note:` There is no need to understand all of the details of PointNet for brevity. It is important to understand that PointNet allows to process 3D data (e.g. PointCloud), but there is many alternative approaches like Graph Neural Networks, 3D Convolutional Neural Networks (voxelization).\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_max_pool\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from torch_geometric.nn import MessagePassing\n",
        "\n",
        "class PointNetLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        # Message passing with \"max\" aggregation.\n",
        "        super().__init__(aggr='max')\n",
        "\n",
        "        # Initialization of the MLP:\n",
        "        # Here, the number of input features correspond to the hidden node\n",
        "        # dimensionality plus point dimensionality (=3).\n",
        "        self.mlp = Sequential(Linear(in_channels + 3, out_channels),\n",
        "                              ReLU(),\n",
        "                              Linear(out_channels, out_channels))\n",
        "\n",
        "    def forward(self, h, pos, edge_index):\n",
        "        # Start propagating messages.\n",
        "        return self.propagate(edge_index, h=h, pos=pos)\n",
        "\n",
        "    def message(self, h_j, pos_j, pos_i):\n",
        "        # h_j defines the features of neighboring nodes as shape [num_edges, in_channels]\n",
        "        # pos_j defines the position of neighboring nodes as shape [num_edges, 3]\n",
        "        # pos_i defines the position of central nodes as shape [num_edges, 3]\n",
        "\n",
        "        input = pos_j - pos_i  # Compute spatial relation.\n",
        "\n",
        "        if h_j is not None:\n",
        "            # In the first layer, we may not have any hidden node features,\n",
        "            # so we only combine them in case they are present.\n",
        "            input = torch.cat([h_j, input], dim=-1)\n",
        "\n",
        "        return self.mlp(input)  # Apply our final MLP.\n",
        "\n",
        "\n",
        "class PointNet(torch.nn.Module):\n",
        "    def __init__(self, emb_dim: int=32):\n",
        "        super().__init__()\n",
        "\n",
        "        torch.manual_seed(42)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_classes = 1\n",
        "        self.conv1 = PointNetLayer(3, emb_dim)\n",
        "        self.conv2 = PointNetLayer(emb_dim, emb_dim)\n",
        "\n",
        "    def forward(self, pos, batch):\n",
        "\n",
        "\n",
        "\n",
        "        # Compute the kNN graph:\n",
        "        # Here, we need to pass the batch vector to the function call in order\n",
        "        # to prevent creating edges between points of different examples.\n",
        "        edge_index = knn_graph(pos.cpu(), k=16, batch=batch.cpu(), loop=True).to(pos.device)\n",
        "        batch = batch.to(pos.device).long()\n",
        "\n",
        "        # 3. Start bipartite message passing.\n",
        "        h = self.conv1(h=pos, pos=pos, edge_index=edge_index)\n",
        "        h = h.relu()\n",
        "        h = self.conv2(h=h, pos=pos, edge_index=edge_index)\n",
        "        h = h.relu()\n",
        "\n",
        "        # 4. Global Pooling.\n",
        "        h = global_max_pool(h, batch=batch)\n",
        "\n",
        "        # representations h\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBaiC1ZDcNGc"
      },
      "outputs": [],
      "source": [
        "#@title Create a quad. encoder net that processes protein and ligand seq+structure data üõ†Ô∏è\n",
        "\n",
        "#@markdown `quad_encoder_net` is a neural network function that processes both Protein sequences and Ligand SMILES strings + protein and ligand point clouds and predicts log $\\tilde{K_d}$ values. The architecture is illustrated with the following figure.\n",
        "\n",
        "#@markdown `Note:` Create a similar architecture as `simple_MLP` for the two seuence encoders. We will introduce two more structure encodes using PointNet model.\n",
        "\n",
        "#@markdown `Second Note:` Each sequence encoder will convert the protein and ligand sequence into a hidden representation $h_s$ or $h_t$. Each structure encoder (i.e. PointNet) will convert the protein and ligand structure into a hidden representation $H_p$ and $H_l$. We will concatenate these representations into $H=[h_p,H_p,h_l,H_l]$ and then process it through a downstream MLP head to predict binding affinities $log\\tilde{K_d}$.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Overall architecture should be the following:\n",
        "\n",
        "#@markdown 1.) $h_p$ ~ $Encoder_{\\theta_p}(x_p)$, $H_p$ ~ $Encoder_{\\omega_p}(X_p)$, $h_l$ ~ $Encoder_{\\theta_l}(x_l)$, $H_l$ ~ $Encoder_{\\omega_l}(X_l)$, where $x_p, X_p, x_l, X_l$ are the protein sequence, protein structure, ligand sequence, and ligand structure input data, $\\theta_p$, $\\omega_p$, $\\theta_l$, $\\omega_l$ are model parameters for protein sequences, protein structures, ligand SMILES strings, and ligand SMILES structures. The $h_p$, $H_p$, $h_l$, and $H_l$ are hidden representations for protein sequences, protein structures, ligand sequences, and ligand structures.\n",
        "\n",
        "#@markdown 2.) H = [$h_p$, $H_p$, $h_l$, $H_l$]; concatenate the representations into an individual tensor\n",
        "\n",
        "#@markdown 3.) y ~ $MLP_{\\theta_b}(H)$, where $\\theta_b$ is the model parameters for the downstream regression head and $y$ is the binding affinity output labels.\n",
        "\n",
        "\n",
        "#@markdown `note:` Please analyze the `__init__()` to understand the variables required to assemble the model and `forward()` to understand the input data that is required to predict binding affinities.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "class quad_encoder_net(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      protein_input_size: int,\n",
        "      protein_hidden_size: int,\n",
        "      protein_depth: int,\n",
        "      ligand_input_size: int,\n",
        "      ligand_hidden_size: int,\n",
        "      ligand_depth: int,\n",
        "      pointnet_emb_dim: int,\n",
        "      head_hidden_emb: int\n",
        "    ):\n",
        "\n",
        "    super(quad_encoder_net, self).__init__()\n",
        "\n",
        "    # define hyperparameters\n",
        "    self.protein_input_size = protein_input_size\n",
        "    self.protein_hidden_size = protein_hidden_size\n",
        "    self.protein_depth = protein_depth\n",
        "    self.ligand_input_size = ligand_input_size\n",
        "    self.ligand_hidden_size = ligand_hidden_size\n",
        "    self.ligand_depth = ligand_depth\n",
        "    self.pointnet_emb_dim = pointnet_emb_dim\n",
        "    self.head_hidden_emb = head_hidden_emb\n",
        "\n",
        "    # define architecture layers\n",
        "    ' protein encoders'\n",
        "    self.protein_seq_layers = self.protein_MLP() # h_p ~ Encoder(h_p | x_p)\n",
        "    self.protein_struct_net = PointNet(emb_dim=self.pointnet_emb_dim)\n",
        "\n",
        "    ' ligand encoders'\n",
        "    self.ligand_seq_layers = self.ligand_MLP() # h_l ~ Encoder(h_l | x_l)\n",
        "    self.ligand_struct_net = PointNet(emb_dim=self.pointnet_emb_dim)\n",
        "\n",
        "    ' top regression head'\n",
        "    self.head_layers = self.downstream_head_MLP() # y ~ MLP(y | h), where h = [h_p, h_l]\n",
        "\n",
        "\n",
        "  def protein_MLP(self,):\n",
        "      'note: similar architecture as simple_MLP without output layer'\n",
        "      protein_layers = nn.ModuleList()\n",
        "\n",
        "      # input layers\n",
        "      protein_layers.append(nn.Linear(\n",
        "          self.protein_input_size, self.protein_hidden_size\n",
        "      ))\n",
        "      protein_layers.append(nn.GELU())\n",
        "\n",
        "      # hidden layers\n",
        "      for _ in range(self.protein_depth - 1):\n",
        "        protein_layers.append(nn.Linear(\n",
        "                    self.protein_hidden_size, self.protein_hidden_size\n",
        "        ))\n",
        "        protein_layers.append(nn.GELU())\n",
        "\n",
        "      return protein_layers\n",
        "\n",
        "  def ligand_MLP(self,):\n",
        "      'note: similar architecture as simple_MLP without output layer'\n",
        "      ligand_layers = nn.ModuleList()\n",
        "\n",
        "      # input layers\n",
        "      ligand_layers.append(nn.Linear(\n",
        "          self.ligand_input_size, self.ligand_hidden_size\n",
        "      ))\n",
        "      ligand_layers.append(nn.GELU())\n",
        "\n",
        "      # hidden layers\n",
        "      for _ in range(self.ligand_depth - 1):\n",
        "        ligand_layers.append(nn.Linear(\n",
        "                    self.ligand_hidden_size, self.ligand_hidden_size\n",
        "        ))\n",
        "        ligand_layers.append(nn.GELU())\n",
        "\n",
        "      return ligand_layers\n",
        "\n",
        "  def downstream_head_MLP(self,):\n",
        "    'note: We will create the architecture for you, but you will need to figure out the right variables for the hyperparameters (e.g. input_dim)'\n",
        "\n",
        "    tot_point_emb_dim = 2*(self.pointnet_emb_dim)\n",
        "    input_dim = tot_point_emb_dim + self.protein_hidden_size + self.ligand_hidden_size # remember h = [h_p, h_l]\n",
        "    output_dim = 1 # regression task\n",
        "\n",
        "    ' MLP head for regression'\n",
        "    head_layers = nn.Sequential(\n",
        "        nn.Linear(input_dim, self.head_hidden_emb),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(self.head_hidden_emb, 1)\n",
        "    )\n",
        "\n",
        "    return head_layers\n",
        "\n",
        "  # forward pass of neural network y ~ f(x)\n",
        "  def forward(self, x_seq_p, x_struct_p, x_seq_l, x_struct_l):\n",
        "\n",
        "    # Flatten the input tensor\n",
        "    x_seq_p = x_seq_p.view(x_seq_p.size(0), -1) # protein sequence data\n",
        "    x_seq_l = x_seq_l.view(x_seq_l.size(0), -1) # ligand sequence data\n",
        "\n",
        "    ' infer hidden representations for proteins sequences '\n",
        "    h_seq_p = x_seq_p.clone()\n",
        "    for layer in self.protein_seq_layers:\n",
        "        h_seq_p = layer(h_seq_p)\n",
        "\n",
        "    ' infer hidden representations for proteins structure '\n",
        "    h_struct_p, batch_size, num_nodes = [], x_struct_p.shape[0], x_struct_p.shape[1]\n",
        "    # Create the batch vector with cyclic pattern\n",
        "    batch = torch.arange(batch_size).repeat_interleave(num_nodes)\n",
        "    # compute structure hidden representation\n",
        "    h_struct_p = self.protein_struct_net(x_struct_p.view(-1, 3), batch)\n",
        "\n",
        "    ' infer hidden representations for ligands sequences '\n",
        "    h_seq_l = x_seq_l.clone()\n",
        "    for layer in self.ligand_seq_layers:\n",
        "        h_seq_l = layer(h_seq_l)\n",
        "\n",
        "    ' infer hidden representations for ligand structure '\n",
        "    h_struct_l, batch_size, num_nodes = [], x_struct_l.shape[0], x_struct_l.shape[1]\n",
        "    # create the batch vector with cyclic pattern\n",
        "    batch = torch.arange(batch_size).repeat_interleave(num_nodes)\n",
        "    # compute structure hidden representation\n",
        "    h_struct_l = self.ligand_struct_net(x_struct_l.view(-1, 3), batch)\n",
        "\n",
        "    # concatenate representations\n",
        "    h = torch.cat((h_seq_p, h_struct_p, h_seq_l, h_struct_l), dim=-1)\n",
        "\n",
        "\n",
        "    ' predict regression values '\n",
        "    y_pred = self.head_layers(h)\n",
        "\n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccdEh2bLeWSi"
      },
      "outputs": [],
      "source": [
        "#@title Problem: define the protein sequence-based model for affinity prediction  üö©\n",
        "\n",
        "\n",
        "'point net structure hyperparameters for both ligand and protein encoders'\n",
        "pointnet_emb_dim = 32 # pointnet hyperparameter; control expressiveness\n",
        "\n",
        "' Protein encoder hyperparameters '\n",
        "protein_seq_len = train_x_protein_seq.shape[1] # sequence length\n",
        "protein_char_tokens = train_x_protein_seq.shape[-1] # number of protein sequence tokens\n",
        "protein_input_size=protein_seq_len * protein_char_tokens # input dimension to MLP\n",
        "protein_hidden_size=500 # hyperparameter; control expressive the neural network is...\n",
        "protein_depth=2 # hyperparameter; control how deep the neural network is...\n",
        "\n",
        "' Ligand encoder hyperparameters '\n",
        "ligand_seq_len = train_x_ligand_seq.shape[1] # sequence length\n",
        "ligand_char_tokens = train_x_ligand_seq.shape[-1] # number of ligand sequence tokens\n",
        "ligand_input_size=ligand_seq_len * ligand_char_tokens # input dimension to MLP\n",
        "ligand_hidden_size=500 # hyperparameter; control expressiveness of the neural network is...\n",
        "ligand_depth=2 # hyperparameter; control how deep the neural network is...\n",
        "\n",
        "' head hyperparameters'\n",
        "head_hidden_emb=2000 # hyperparameter; control expressive the neural network is..\n",
        "\n",
        "\n",
        "# Problem: create model by filling in the hyperparameters\n",
        "BioMol_seq_struct_model = quad_encoder_net(\n",
        "    protein_input_size=protein_input_size,\n",
        "    protein_hidden_size=protein_hidden_size,\n",
        "    protein_depth=protein_depth,\n",
        "    ligand_input_size=ligand_input_size,\n",
        "    ligand_hidden_size=ligand_hidden_size,\n",
        "    ligand_depth=ligand_depth,\n",
        "    pointnet_emb_dim=pointnet_emb_dim,\n",
        "    head_hidden_emb=head_hidden_emb,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7Snwp58Tekq5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Function for compute metrics on test set for quad. encoder üõ†Ô∏è\n",
        "@torch.no_grad()\n",
        "def test_performance_metrics_on_quad(model: nn.Module, dataloader: DataLoader):\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    # Define a loss function\n",
        "    criterion = nn.MSELoss()  # For simple regression prediction, we can use mean_squared_error loss.\n",
        "\n",
        "    y_true_batch, y_pred_batch = [], []\n",
        "\n",
        "    for i, (x_seq_p, x_struct_p, x_seq_l, x_struct_l, y_true) in enumerate(dataloader):\n",
        "\n",
        "        # Move inputs and labels to the device\n",
        "        x_seq_p, x_struct_p, x_seq_l, x_struct_l, y_true = x_seq_p.to(device), x_struct_p.to(device), x_seq_l.to(device), x_struct_l.to(device), y_true.to(device)\n",
        "\n",
        "\n",
        "        # Forward propagation\n",
        "        y_pred = model(\n",
        "            x_seq_p=x_seq_p,\n",
        "            x_struct_p=x_struct_p,\n",
        "            x_seq_l=x_seq_l,\n",
        "            x_struct_l=x_struct_l\n",
        "        ).squeeze(1)\n",
        "\n",
        "        y_true_batch.append(y_true)\n",
        "        y_pred_batch.append(y_pred)\n",
        "\n",
        "        # concatenate batches\n",
        "        y_true_batch = torch.cat(y_true_batch, dim=0)\n",
        "        y_pred_batch = torch.cat(y_pred_batch, dim=0)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = criterion(y_pred_batch, y_true_batch)\n",
        "\n",
        "        # get metric performance\n",
        "        pearson_corr, spearman_rho = performance_metrics(\n",
        "                    y_true=y_true_batch,\n",
        "                    y_pred=y_pred_batch\n",
        "        )\n",
        "        print(f\"Test metrics | Loss: {loss.item():.3f}, Pearson: {pearson_corr.item():.3f}, Spearman rho: {spearman_rho.item():.3f}\")\n",
        "\n",
        "\n",
        "        return (\n",
        "          loss.item(),\n",
        "          pearson_corr.item(),\n",
        "          spearman_rho.item()\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "26odqSJxemwJ"
      },
      "outputs": [],
      "source": [
        "#@title Train the Sequence-Structure model üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
        "\n",
        "# Define a loss function\n",
        "criterion = nn.MSELoss()  # For simple regression prediction, we can use mean_squared_error loss.\n",
        "\n",
        "# define an optimizer\n",
        "optimizer = optim.Adam(BioMol_seq_struct_model.parameters(), lr=0.0001)  # lr corresponds to learning_rate (keep fixed for now)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BioMol_seq_struct_model = BioMol_seq_struct_model.to(device)\n",
        "\n",
        "# number of training cycles over the dataset (i.e. epochs)\n",
        "num_epochs=1#@param{type:\"integer\"}\n",
        "\n",
        "\n",
        "# track losses and metrics\n",
        "train_seq_struct_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "test_seq_struct_history_dict = {\n",
        "    'step': [],\n",
        "    'loss': [],\n",
        "    'pearson_corr': [],\n",
        "    'spearman_rho': []\n",
        "}\n",
        "\n",
        "iteration = 0\n",
        "# Start the training loop\n",
        "for epoch in range(num_epochs):  # num_epochs is the number of epochs to train for\n",
        "    for i, (x_seq_p, x_struct_p, x_seq_l, x_struct_l, y_true) in enumerate(train_seq_struct_dataloader ):\n",
        "        # Move inputs and labels to the device\n",
        "        x_seq_p, x_struct_p, x_seq_l, x_struct_l, y_true = x_seq_p.to(device), x_struct_p.to(device), x_seq_l.to(device), x_struct_l.to(device), y_true.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        y_pred = BioMol_seq_struct_model(\n",
        "            x_seq_p=x_seq_p,\n",
        "            x_struct_p=x_struct_p,\n",
        "            x_seq_l=x_seq_l,\n",
        "            x_struct_l=x_struct_l,\n",
        "        ).squeeze(1)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "        pearson_corr, spearman_rho = performance_metrics(\n",
        "                y_true=y_true,\n",
        "                y_pred=y_pred\n",
        "        )\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_seq_struct_dataloader)}, Loss: {loss.item():.3f}, Pearson: {pearson_corr.item():.3f}, Spearman rho: {spearman_rho.item():.3f}\")\n",
        "\n",
        "\n",
        "        # log training performance\n",
        "        train_seq_struct_history_dict['step'].append(iteration)\n",
        "        iteration+=1\n",
        "        train_seq_struct_history_dict['loss'].append(loss.item())\n",
        "        train_seq_struct_history_dict['pearson_corr'].append(pearson_corr.item())\n",
        "        train_seq_struct_history_dict['spearman_rho'].append(spearman_rho.item())\n",
        "\n",
        "    # compute performance metrics on the test set\n",
        "    test_loss, test_pearson_corr, test_spearman_rho = test_performance_metrics_on_quad(model=BioMol_seq_struct_model, dataloader=test_seq_struct_dataloader)\n",
        "    test_seq_struct_history_dict['step'].append(iteration)\n",
        "    test_seq_struct_history_dict['loss'].append(test_loss)\n",
        "    test_seq_struct_history_dict['pearson_corr'].append(test_pearson_corr)\n",
        "    test_seq_struct_history_dict['spearman_rho'].append(test_spearman_rho)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svVwUtzNhcgf"
      },
      "outputs": [],
      "source": [
        "#@title Plot training + testing history\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10,3))\n",
        "\n",
        "axes[0].plot(train_seq_struct_history_dict['step'], train_seq_struct_history_dict['loss'])\n",
        "axes[0].plot(test_seq_struct_history_dict['step'], test_seq_struct_history_dict['loss'], '.-', markersize=10)\n",
        "axes[0].set_ylabel('loss')\n",
        "axes[0].set_xlabel('steps')\n",
        "\n",
        "axes[1].plot(train_seq_struct_history_dict['step'], train_seq_struct_history_dict['pearson_corr'])\n",
        "axes[1].plot(test_seq_struct_history_dict['step'], test_seq_struct_history_dict['pearson_corr'], '.-', markersize=10)\n",
        "axes[1].set_ylabel('Pearson corr.')\n",
        "axes[1].set_xlabel('steps')\n",
        "\n",
        "axes[2].plot(train_seq_struct_history_dict['step'], train_seq_struct_history_dict['spearman_rho'])\n",
        "axes[2].plot(test_seq_struct_history_dict['step'], test_seq_struct_history_dict['spearman_rho'], '.-', markersize=10)\n",
        "axes[2].set_ylabel('Spearman rho')\n",
        "axes[2].set_xlabel('steps')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZzoFIT8quKoT"
      },
      "outputs": [],
      "source": [
        "#@title Compare performance between the 3 approaches on the test set.\n",
        "\n",
        "#pretrained_p_seq_test_dict = torch.load('/content/UChicago_AIplusScience_code_workshops/pretrained_weights/test_Protein_sequence_history.pt')\n",
        "#pretrained_l_seq_test_dict = torch.load('/content/UChicago_AIplusScience_code_workshops/pretrained_weights/test_Ligand_sequence_history.pt')\n",
        "#pretrained_pl_seq_test_dict = torch.load('/content/UChicago_AIplusScience_code_workshops/pretrained_weights/test_Protein_Ligand_sequence_history.pt')\n",
        "pretrained_pl_seq_struct_test_dict = torch.load('/content/UChicago_AIplusScience_code_workshops/pretrained_weights/test_Sequene_Structure_history.pt')\n",
        "\n",
        "\n",
        "plt.bar(\n",
        "    ['ligand encoder',\n",
        "     'protein encoder',\n",
        "     'protein-ligand \\n dual encoder',\n",
        "     'protein-ligand \\n seq-struct. encoders'],\n",
        "    [\n",
        "        pretrained_p_seq_test_dict['spearman_rho'][-1],\n",
        "        pretrained_l_seq_test_dict['spearman_rho'][-1],\n",
        "        pretrained_pl_seq_test_dict['spearman_rho'][-1],\n",
        "        pretrained_pl_seq_struct_test_dict['spearman_rho'][-1]\n",
        "        ]\n",
        ")\n",
        "plt.ylabel('Spearmann rho on test set.')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6wvwXZdtK6M"
      },
      "source": [
        "## Extension examples:\n",
        "- docking then use point cloud representations\n",
        "- finding new protein-ligand through your predictor.\n",
        "- soph. architectures (e.g. graph neural networks for structure or transformers for sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFEI4iJvtXok"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2VyIkRK08B4THVEI/MlcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}